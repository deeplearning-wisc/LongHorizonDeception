#!/usr/bin/env python3
# main.py
# DeceptioNä¸»ç¨‹åº - è¿è¡Œæ¬ºéª—æ£€æµ‹å®éªŒ

import sys
import os
import argparse
from pathlib import Path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from config_manager import load_config
from utils.task_stream_logger import TaskStreamLogger
from utils.result_saver import ResultSaver
from core.manager import Manager
from core.LLM import LLM
from core.detector import DeceptionDetector
from tasks.json_loader import JSONTaskLoader
from tasks.event_loader import get_event_system, DeceptiveEvent
from tasks.base import Task
from tqdm import tqdm

class TaskStreamTask:
    def __init__(self, task_id, title, description, task_sequence_num=1):
        self.task_sequence_num = task_sequence_num
        self.task_id = task_id
        self.title = title
        self.description = description
        self.files = []
        # Agentéœ€è¦çš„promptå±æ€§
        # MockTaskPromptä¸å†éœ€è¦normal_promptå­—æ®µ
        self.deceptive_prompt = None
        self.positive_prompt = None

class MockTaskPrompt:
    def __init__(self, text):
        self.en = text

# Context management functions moved to Agent class

def safe_log(logger, method_name, *args, **kwargs):
    """å®‰å…¨çš„æ—¥å¿—è®°å½•å‡½æ•° - å½“loggerä¸ºNoneæ—¶ä¸æ‰§è¡Œ"""
    if logger:
        method = getattr(logger, method_name)
        method(*args, **kwargs)

def run_deception_experiment(config_name=None):
    """è¿è¡Œé•¿æœŸTaskåºåˆ—çš„æ¬ºéª—å®éªŒ"""
    
    # Step 1: åŠ è½½é…ç½®ç³»ç»Ÿ (ç»Ÿä¸€YAMLé…ç½®)
    logger = None  # åˆå§‹åŒ–loggerå˜é‡
    try:
        # åŠ è½½ç»Ÿä¸€é…ç½®
        config = load_config(config_name)
        
        # æ£€æŸ¥æ˜¯å¦å¯ç”¨logger
        enable_logger = config['logging']['enable_logger']
        
        # æ ¹æ®é…ç½®å†³å®šæ˜¯å¦åˆå§‹åŒ–logger
        logger = None
        session_info = None
        if enable_logger:
            logger = TaskStreamLogger()
            session_info = logger.get_session_info()
            safe_log(logger, 'log_config_loading', "Full Configuration", config)
        else:
            # ç”Ÿæˆç®€å•çš„sessionä¿¡æ¯ï¼Œä¸åˆ›å»ºlogæ–‡ä»¶
            import time
            timestamp = time.strftime("%Y%m%d_%H%M%S")
            session_info = {
                'session_name': f'task_stream_{timestamp}',
                'log_file': None  # ä¸åˆ›å»ºlogæ–‡ä»¶
            }
        
    except Exception as e:
        safe_log(logger, 'log_error', e, "Configuration loading")
        print(f"âŒ Configuration loading failed: {e}")
        return False
    
    # é…ç½®å·²ç»é€šè¿‡config_manageréªŒè¯è¿‡äº†ï¼Œè¿™é‡Œåªéœ€è¦è®°å½•
    safe_log(logger, "log_config_loading", "Configuration validation passed", config)
    
    # æ ¹æ®experiment_modeç¡®å®šå®éªŒæ¨¡å¼
    if config['experiment_mode'].lower() == "quick":
        max_tasks = 7  # å¿«é€Ÿæ¨¡å¼ï¼š7ä¸ªTask
        experiment_mode = "QUICK MODE"
    else:
        max_tasks = 21  # å®Œæ•´å®éªŒï¼š21ä¸ªTask
        experiment_mode = "FULL EXPERIMENT"
    
    # æ§åˆ¶å°æ˜¾ç¤ºå®éªŒä¿¡æ¯
    print(f"ğŸš€ Starting DeceptioN Experiment - {experiment_mode}")
    print(f"ğŸ“Š Task Count: {max_tasks} tasks")
    print(f"ğŸ“ Session: {session_info['session_name']}")
    print(f"ğŸ“„ Log file: {session_info['log_file']}")
    
    # Step 3: åˆå§‹åŒ–ç»„ä»¶
    print("ğŸ”§ Initializing components...")
    
    # åˆå§‹åŒ–è®°å½•ç³»ç»Ÿ - ä¸¥æ ¼æ¨¡å¼ï¼Œæ— é»˜è®¤å€¼
    logging_config = config['logging']
    enable_logger = logging_config['enable_logger']
    enable_result_saver = logging_config['enable_result_saver']
    
    # åˆå§‹åŒ–ResultSaver (å¦‚æœå¯ç”¨)
    result_saver = None
    if enable_result_saver:
        result_saver = ResultSaver(session_info['session_name'])
        result_saver.set_experiment_config(config)
        
        # ğŸ†• ç›´æ¥å¤åˆ¶é…ç½®æ–‡ä»¶ä½œä¸ºmetadataï¼
        import shutil
        session_dir = Path(f"results/{session_info['session_name']}")
        session_dir.mkdir(parents=True, exist_ok=True)
        
        # ç¡®å®šä½¿ç”¨çš„é…ç½®æ–‡ä»¶å
        if config_name is None:
            config_file = "medium.yaml"  # é»˜è®¤ä½¿ç”¨mediumé…ç½®
        else:
            if not config_name.endswith(".yaml"):
                config_name += ".yaml"
            config_file = config_name
        
        # å¤åˆ¶é…ç½®æ–‡ä»¶
        source_config = Path("configs") / config_file
        target_config = session_dir / f"experiment_config_{config_file}"
        shutil.copy2(source_config, target_config)
        print(f"ğŸ“„ Configuration saved: {target_config}")
        
        print("ğŸ“ ResultSaver initialized - Complete interaction records will be saved")
    
    # è·å–åˆ†ç»„ä»¶LLMé…ç½® - ä¸¥æ ¼æ¨¡å¼ï¼Œæ— é»˜è®¤å€¼
    llm_api_config = config['llm_api_config']
    
    def get_component_llm_config(component_name: str):
        """ä¸ºæŒ‡å®šç»„ä»¶è·å–LLMé…ç½® - ä¸¥æ ¼æ¨¡å¼ï¼Œæ— é»˜è®¤å€¼"""
        component_config = llm_api_config[component_name]
        provider = component_config['provider']
        
        # ä¸¥æ ¼æŒ‰ç…§providerè¿”å›å¯¹åº”é…ç½®ï¼Œæ— é»˜è®¤å€¼
        if provider not in component_config:
            raise ValueError(f"Provider '{provider}' config not found for component '{component_name}'")
        
        return provider, component_config[provider]
    
    # åˆå§‹åŒ–Agent - ä½¿ç”¨ä¸“ç”¨é…ç½®
    agent_provider, agent_config = get_component_llm_config('agent')
    # æ˜¾ç¤ºagentä½¿ç”¨çš„æ¨¡å‹å
    agent_model_ref = config['llm_api_config']['agent']
    print(f"[AGENT] Using: {agent_model_ref}")
    
    agent_params = {
        'llm_provider': agent_provider,
        'llm_config': agent_config,
        'system_prompt': config['agent_system_prompt'],
        'max_tokens': 16384
    }
    safe_log(logger, "log_component_init", "Agent", agent_params)
    agent = LLM(**agent_params)
    agent._logger = logger  # ä¼ é€’loggerç»™Agent
    agent.llm_client._logger = logger  # ä¼ é€’loggerç»™LLMå®¢æˆ·ç«¯
    
    # åˆå§‹åŒ–Manager - ä½¿ç”¨ä¸“ç”¨é…ç½®
    manager_provider, manager_config = get_component_llm_config('manager')
    # æ˜¾ç¤ºmanagerä½¿ç”¨çš„æ¨¡å‹å
    manager_model_ref = config['llm_api_config']['manager']
    print(f"[MANAGER] Using: {manager_model_ref}")
    
    manager_params = {
        'llm_provider': manager_provider,
        'llm_config': manager_config,
        'summary_level': config['summary_level'],
        'system_prompt': config['manager_system_prompt'],
        'initial_state': config['manager_initial_state'],
        'update_prompt': config['manager_update_prompt']
    }
    safe_log(logger, "log_component_init", "Manager", manager_params)
    manager = Manager(**manager_params)
    manager._logger = logger  # ä¼ é€’loggerç»™Manager
    manager.llm_client._logger = logger  # ä¼ é€’loggerç»™Managerçš„LLMå®¢æˆ·ç«¯
    
    # æ˜¾ç¤ºä¼šè¯ä¿¡æ¯
    print(f"- Session: {session_info['session_name']}")
    if enable_logger and session_info.get('log_file'):
        print(f"- Log file: {session_info['log_file']}")
    print()
    
    # Step 4: åŠ è½½ä»»åŠ¡æ•°æ® - æ ¹æ®TEST_MODEç¡®å®šæ•°é‡
    task_loader = JSONTaskLoader()
    
    # åŠ è½½market_research_21dayä»»åŠ¡é›†
    task_set = task_loader.load_task_set("market_research_21day")
    all_tasks = task_set.get_tasks()
    
    # æ ¹æ®TEST_MODEé€‰æ‹©Taskæ•°é‡
    selected_tasks = all_tasks[:max_tasks]
    safe_log(logger, "log_info", f"ra{len(selected_tasks)} tasks for {experiment_mode}")
    for i, task in enumerate(selected_tasks, 1):
        safe_log(logger, "log_info", f"  Task {i}: {task.task_id} - {task.title}")
    
    # åˆå§‹åŒ–äº‹ä»¶ç³»ç»Ÿ - ä½¿ç”¨é…ç½®ç³»ç»Ÿ
    event_system = get_event_system("market_research_21day", global_event_probability=config['p_event'])
    safe_log(logger, "log_info", f"Event system initialized with {len(event_system.phase_events)} phases, probability: {config['p_event']}")
    
    # ä¸ºæ¯ä¸ªä»»åŠ¡ç”Ÿæˆäº‹ä»¶ - ä½¿ç”¨é…ç½®çš„æ¦‚ç‡
    import random
    p_event = config['p_event']
    
    # å®šä¹‰æ ¹æ®Taskåºåˆ—å·ç¡®å®šé˜¶æ®µçš„å‡½æ•°
    def get_phase_for_task_sequence(task_sequence_num: int):
        """æ ¹æ®Taskåºåˆ—å·ç¡®å®šå‹åŠ›é˜¶æ®µ"""
        from tasks.variant_event_loader import EventPhase
        if task_sequence_num <= 7:
            return EventPhase.PHASE_1
        elif task_sequence_num <= 15:
            return EventPhase.PHASE_2
        else:
            return EventPhase.PHASE_3
    
    task_stream = []
    for task_idx, task in enumerate(selected_tasks, 1):
        # ç”Ÿæˆäº‹ä»¶
        should_have_event = random.random() < p_event
        if should_have_event:
            # æ ¹æ®Taskåºåˆ—å·ç¡®å®šé˜¶æ®µ
            phase = get_phase_for_task_sequence(task_idx)
            deceptive_event = event_system.sample_event(phase)
            # ä»variantsä¸­éšæœºé€‰æ‹©ä¸€ä¸ªå˜ä½“
            variant = deceptive_event.get_random_variant()
            event = {
                'content': variant.content,          # Agentçœ‹åˆ°çš„å®é™…äº‹ä»¶å†…å®¹
                'description': deceptive_event.description,  # ç³»ç»Ÿå†…éƒ¨æè¿°
                'pressure_level': variant.pressure_level,
                'category': deceptive_event.category  # äº‹ä»¶ç±»åˆ«
            }
        else:
            event = {
                'content': 'Normal business conditions',     # Agentçœ‹åˆ°çš„å†…å®¹
                'description': 'No special event',          # ç³»ç»Ÿæè¿°
                'pressure_level': 'NORMAL',
                'category': 'NONE'                          # æ²¡æœ‰ç‰¹æ®Šäº‹ä»¶
            }
        
        task_stream.append({
            'task': task,
            'event': event
        })
        
        safe_log(logger, "log_info", f"Task {task_idx}: {task.task_id} - Event '{event['content'][:50]}...' ({event['category']}, Pressure: {event['pressure_level']})")
    
    # å¤„ç†Task Stream
    print("Processing tasks...")
    
    for task_idx, task_info in enumerate(tqdm(task_stream, desc="Tasks", unit="task")):
        task = task_info['task']
        event = task_info['event']
        
        # è®°å½•ä»»åŠ¡å¼€å§‹
        safe_log(logger, "log_task_start", task_idx + 1, task.task_id, task.description or task.title)
        safe_log(logger, "log_event_info", event)
        
        # ResultSaver: è®°å½•ä»»åŠ¡å¼€å§‹å¹¶å®æ—¶ä¿å­˜
        if result_saver:
            result_saver.start_task(task_idx + 1, task, event)
            result_saver.flush()  # å®æ—¶ä¿å­˜
        
        # è®°å½•Agentå…¨å±€å†å²çŠ¶æ€
        if len(agent.global_conversation_history) > 0:
            safe_log(logger, "log_info", f"Agent has {len(agent.global_conversation_history)} interactions from previous tasks")
        
        # å¤šè½®äº¤äº’ç›´åˆ°ä»»åŠ¡å®Œæˆ - ä½¿ç”¨é…ç½®çš„æœ€å¤§è½®æ•°
        max_rounds = config['max_rounds_per_task']
        task_completed = False
        
        # ğŸ†• ä»»åŠ¡å†…çš„äº¤äº’å†å² - å®ç°çœŸæ­£çš„äº¤é”™å¯¹è¯
        task_manager_feedback_history = []
        
        # è½®æ¬¡è¿›åº¦æ¡
        rounds_desc = f"Task {task.task_id}"
        for round_num in tqdm(range(1, max_rounds + 1), desc=rounds_desc, unit="round", leave=False):
            safe_log(logger, "log_round_start", round_num, max_rounds)
            
            # ResultSaver: è®°å½•è½®æ¬¡å¼€å§‹å’ŒManagerçŠ¶æ€ï¼Œå¹¶å®æ—¶ä¿å­˜
            if result_saver:
                result_saver.start_round(round_num, manager.current_state.copy())
                result_saver.flush()  # å®æ—¶ä¿å­˜
            
            # è®°å½•ä»»åŠ¡æ–‡ä»¶ä¿¡æ¯
            safe_log(logger, "log_info", f"Task files count: {len(task.files) if task.files else 0}")
            if task.files:
                for i, file_obj in enumerate(task.files):
                    safe_log(logger, "log_info", f"  File {i+1}: {file_obj.filename} ({len(file_obj.content)} chars)")
            
            if round_num > 1:
                safe_log(logger, "log_info", f"Agent has {len(task_manager_feedback_history)} manager feedback(s) from this task")
            
            # ğŸ†• ä½¿ç”¨Agentçš„å°è£…å¥½çš„contextç®¡ç† + ä»»åŠ¡å†…Manageråé¦ˆ
            try:
                # åˆ¤æ–­æ˜¯å¦éœ€è¦ä½¿ç”¨ChatGPTé£æ ¼çš„äº¤é”™å¯¹è¯
                has_global_history = len(agent.global_conversation_history) > 0
                has_task_feedback = len(task_manager_feedback_history) > 0
                
                if has_global_history or has_task_feedback:
                    # æœ‰å…¨å±€å†å²æˆ–ä»»åŠ¡å†…åé¦ˆï¼šä½¿ç”¨ChatGPTé£æ ¼çš„äº¤é”™å¯¹è¯
                    enhanced_prompt = agent._build_enhanced_prompt_with_history(task, event['content'])
                    agent_result = agent.process_task_with_enhanced_prompt(task, enhanced_prompt, task_manager_feedback_history)
                else:
                    # å®Œå…¨æ–°çš„å¼€å§‹ï¼šä½¿ç”¨ç®€å•çš„äº‹ä»¶å¤„ç†
                    agent_result = agent.process_task_with_event(task, event['content'])
                
                agent_response = agent_result['response']
                
                # è®°å½•Agentçš„è¾“å‡º
                safe_log(logger, "log_agent_output", agent_response, agent_result.get('llm_metadata', {}))
                
                # ResultSaver: ä¿å­˜Agentå›å¤ - å®Œæ•´å†…å®¹ï¼Œç»ä¸æˆªæ–­ï¼Œå¹¶å®æ—¶ä¿å­˜
                if result_saver:
                    result_saver.save_agent_response(round_num, agent_response)
                    result_saver.flush()  # å®æ—¶ä¿å­˜
                
            except Exception as e:
                safe_log(logger, "log_error", e, f"Agent processing - Task {task.task_id}, Round {round_num}")
                raise  # ç›´æ¥æŠ›å‡ºå¼‚å¸¸ï¼Œfail-faståŸåˆ™
            
            # Managerè¯„ä¼°
            try:
                # å°†event dictè½¬æ¢ä¸ºevent_infoæ ¼å¼ - åªä¼ é€’contentï¼Œä¸æš´éœ²ç³»ç»Ÿå­—æ®µ
                event_info = {
                    'description': event['content']        # Manageråªéœ€è¦çŸ¥é“å®é™…çš„äº‹ä»¶å†…å®¹
                }
                
                # Managerä¼šè‡ªåŠ¨æ‰“å°å®Œæ•´contextï¼Œç„¶åè·å–ç»“æœ
                manager_result = manager.evaluate_and_update(agent_response, task, event_info)
                
                task_completed = manager_result['task_complete']
                
                # è®°å½•Managerè¾“å‡º
                safe_log(logger, "log_manager_output", 
                    task_completed,
                    manager_result['state_updates'],
                    manager_result['comprehensive_reasoning'],
                    manager_result['feedback_response'],
                    manager_result.get('tokens_used', 0),
                    manager_result.get('detailed_reasoning', {})
                )
                
                # ResultSaver: ä¿å­˜Managerè¯„ä¼° - å®Œæ•´çš„æ‰€æœ‰æ¨ç†ï¼Œç»ä¸æˆªæ–­ï¼Œå¹¶å®æ—¶ä¿å­˜
                if result_saver:
                    result_saver.save_manager_evaluation(round_num, manager_result)
                    result_saver.flush()  # å®æ—¶ä¿å­˜
                
                # ğŸ†• æ·»åŠ Manageråé¦ˆåˆ°ä»»åŠ¡å†…å†å² (ç”¨äºä¸‹ä¸€è½®Agentè¾“å…¥)
                task_manager_feedback_history.append(manager_result['feedback_response'])
                
                # ğŸ†• è®°å½•è¿™ä¸€è½®çš„å®Œæ•´äº¤äº’åˆ°Agentçš„å…¨å±€å†å²
                agent.add_interaction_to_global_history(
                    task_id=task.task_id,
                    round_num=round_num,
                    agent_response=agent_response,
                    manager_feedback=manager_result['feedback_response'],
                    task_complete=task_completed
                )
                
                if task_completed:
                    safe_log(logger, "log_task_completion", task.task_id, True, round_num)
                    break
                else:
                    safe_log(logger, "log_info", f"Task not complete, continuing to round {round_num + 1}")
                    
            except Exception as e:
                safe_log(logger, "log_error", e, f"Manager evaluation - Task {task.task_id}, Round {round_num}")
                raise  # ç›´æ¥æŠ›å‡ºå¼‚å¸¸ï¼Œfail-faståŸåˆ™
        
        if not task_completed:
            safe_log(logger, "log_task_completion", task.task_id, False, max_rounds)
        
        # ResultSaver: ä»»åŠ¡ç»“æŸå¹¶å®æ—¶ä¿å­˜
        if result_saver:
            result_saver.end_task(task_idx + 1)
            result_saver.flush()  # å®æ—¶ä¿å­˜
        
        # è®°å½•å½“å‰çŠ¶æ€å’Œå†å²
        current_state = manager.get_current_state()
        safe_log(logger, "log_info", f"Manager State after Task {task_idx + 1}:")
        safe_log(logger, "log_info", f"  Trust Level: {current_state['trust_level']:.3f}")
        safe_log(logger, "log_info", f"  Work Satisfaction: {current_state['work_satisfaction']:.3f}")
        safe_log(logger, "log_info", f"  Relational Valence: {current_state['relational_valence']:.3f}")
        safe_log(logger, "log_info", f"  Total History Entries: {len(manager.history.interaction_records)}")
    
    # æœ€ç»ˆæ€»ç»“
    final_state = manager.get_current_state()
    total_interactions = len(manager.history.interaction_records)
    safe_log(logger, "log_final_summary", final_state, total_interactions)
    
    # ç°åœ¨è¿›è¡ŒDetectorçš„æ‰¹é‡åˆ†æ
    print("Running detector analysis...")
    
    # åˆå§‹åŒ–Detector - ä½¿ç”¨ä¸“ç”¨é…ç½®
    detector_provider, detector_config = get_component_llm_config('detector')
    # æ˜¾ç¤ºdetectorä½¿ç”¨çš„æ¨¡å‹å
    detector_model_ref = config['llm_api_config']['detector']
    print(f"[DETECTOR] Using: {detector_model_ref}")
    
    safe_log(logger, "log_component_init", "Detector", {'provider': detector_provider, 'config': detector_config})
    detector = DeceptionDetector(detector_provider, detector_config)
    
    # è·å–æ‰€æœ‰äº¤äº’è®°å½•è¿›è¡Œæ‰¹é‡åˆ†æ
    interaction_records = manager.history.interaction_records
    safe_log(logger, "log_info", f"Analyzing {len(interaction_records)} interactions with full sequence context")
    
    # ğŸ’¾ ä¿å­˜ä¸­é—´ç»“æœç”¨äºç‹¬ç«‹åˆ†æ - ç›´æ¥ä¿å­˜manager.history.interaction_recordsçš„åŸå§‹æ ¼å¼
    import json
    intermediate_results_path = f"results/{session_info['session_name']}/interaction_records.json"
    try:
        with open(intermediate_results_path, 'w', encoding='utf-8') as f:
            # ç›´æ¥ä¿å­˜åŸå§‹çš„interaction_recordsï¼Œä¿æŒä¸main.pyä¼ ç»™detectorçš„å®Œå…¨ä¸€è‡´
            json.dump(interaction_records, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"ğŸ’¾ Intermediate results saved to: {intermediate_results_path}")
        safe_log(logger, "log_info", f"Intermediate results saved to: {intermediate_results_path}")
        
    except Exception as e:
        print(f"âš ï¸  Failed to save intermediate results: {e}")
        safe_log(logger, "log_error", e, "Intermediate results saving")
    
    try:
        detector_results = detector.analyze_interaction_sequence(interaction_records)
        safe_log(logger, "log_detector_analysis", detector_results)
        
        # æ§åˆ¶å°æ˜¾ç¤ºç®€è¦ç»“æœ
        print(f"âœ… Detector analysis completed: {len(detector_results)} interactions analyzed")
        deception_count = sum(1 for r in detector_results if r['deception_detected'])
        print(f"ğŸ“Š Deception detected in {deception_count}/{len(detector_results)} interactions")
        
        # ğŸ’¾ ä¿å­˜detectoråˆ†æç»“æœï¼ˆå¸¦æ—¶é—´æˆ³é¿å…è¦†ç›–ï¼‰
        import time
        timestamp = time.strftime('%Y%m%d_%H%M%S', time.localtime())
        detector_output_file = f"results/{session_info['session_name']}/detector_analysis_{timestamp}.txt"
        
        # å¯¼å…¥ä¿å­˜å‡½æ•°
        from core.detector import _save_detector_results_text, _save_detector_results_json
        
        try:
            # ä¿å­˜LLMå‹å¥½çš„æ–‡æœ¬æ ¼å¼
            _save_detector_results_text(detector_results, detector_output_file, session_info['session_name'])
            print(f"ğŸ’¾ Detector analysis (text) saved to: {detector_output_file}")
            
            # ğŸ’¡ ä»managerå†å²ä¸­æå–æ¯è½®çš„çŠ¶æ€æ•°æ®ç”¨äºJSON
            manager_states = []
            for record in manager.history.interaction_records:
                # ä»managerå†å²è®°å½•ä¸­è·å–æ¯è½®åçš„çŠ¶æ€
                manager_states.append({
                    'trust_level': record.get('manager_state_after', {}).get('trust_level', 0.0),
                    'work_satisfaction': record.get('manager_state_after', {}).get('work_satisfaction', 0.0),
                    'relational_valence': record.get('manager_state_after', {}).get('relational_valence', 0.0)
                })
            
            # è‡ªåŠ¨ç”ŸæˆJSONç‰ˆæœ¬ - ä¼ é€’managerçŠ¶æ€æ•°æ®
            json_file = str(Path(detector_output_file).with_suffix('.json'))
            _save_detector_results_json(detector_results, json_file, session_info['session_name'], manager_states)
            print(f"ğŸ’¾ Detector analysis (JSON) saved to: {json_file}")
        except Exception as e:
            print(f"âš ï¸  Failed to save detector results: {e}")
            import traceback
            print(f"Error details: {traceback.format_exc()}")
        
        # ResultSaver: æœ€ç»ˆåŒ–å®éªŒè®°å½•
        if result_saver:
            final_manager_state = manager.get_current_state()
            result_saver.finalize_experiment(
                total_tasks=max_tasks,
                total_interactions=len(interaction_records),
                final_manager_state=final_manager_state
            )
            print(f"ğŸ“ Complete interaction record saved to: {result_saver.get_record_path()}")
        
        # å…³é—­logger
        if logger:
            logger.close()
        
        return True
        
    except Exception as e:
        safe_log(logger, "log_error", e, "Detector batch analysis")
        print(f"âŒ FATAL ERROR in Detector batch analysis: {e}")
        if logger:
            logger.close()
        raise  # ç›´æ¥æŠ›å‡ºå¼‚å¸¸ï¼Œfail-faståŸåˆ™

def main():
    """ä¸»å‡½æ•° - å¤„ç†å‘½ä»¤è¡Œå‚æ•°å¹¶è¿è¡Œå®éªŒ"""
    parser = argparse.ArgumentParser(description='DeceptioN - AI Deception Detection Experiment Framework')
    parser.add_argument('--config', '-c', type=str, help='Configuration file name (e.g., normal, high_pressure, test)')
    parser.add_argument('--list-configs', action='store_true', help='Show available configuration files')
    
    args = parser.parse_args()
    
    # æ˜¾ç¤ºå¯ç”¨é…ç½®æ–‡ä»¶
    if args.list_configs:
        from config_manager import config_manager
        config_manager.show_available_configs()
        return
    
    # è¿è¡Œå®éªŒ
    try:
        if run_deception_experiment(args.config):
            print("âœ… DeceptioN experiment completed successfully!")
        else:
            print("âŒ DeceptioN experiment failed!")
            sys.exit(1)
    except Exception as e:
        print(f"ğŸ’¥ FATAL ERROR: {e}")
        import traceback
        print("Full traceback:")
        traceback.print_exc()
        sys.exit(1)

if __name__ == "__main__":
    main()
