# core/agent.py
# LDLE Agent abstraction layer with unified LLM client

from typing import Dict, Any, Optional, List
from tasks.base import Task, TaskFile
from .llm_client import UnifiedLLMClient
from utils.error_handler import PipelineErrorHandler, RetryHandler

class LDLEAgent:
    """
    LDLE Agent that can process tasks with file attachments
    Uses unified LLM client for robust token handling
    """
    
    def __init__(self, azure_api_key: str, azure_endpoint: str, azure_deployment: str, model_name: str, system_prompt: str, max_tokens: int, api_version: str):
        """
        Initialize the LDLE Agent
        
        Args:
            azure_api_key: Azure OpenAI API key
            azure_endpoint: Azure OpenAI endpoint
            azure_deployment: Azure deployment name
            model_name: Model to use for task processing
            system_prompt: System prompt for the agent
            max_tokens: Maximum tokens for responses
            api_version: Azure API version
        """
        self.azure_api_key = azure_api_key
        self.azure_endpoint = azure_endpoint
        self.azure_deployment = azure_deployment
        self.model_name = model_name
        
        if system_prompt is None:
            raise ValueError("system_prompt is required and cannot be None")
        self.system_prompt = system_prompt
        
        self.max_tokens = max_tokens
        self.api_version = api_version
        
        # ðŸ†• æ·»åŠ é‡è¯•å¤„ç†å™¨
        self.retry_handler = RetryHandler(max_retries=3)
        
        # Use unified LLM client - supports multi-round concatenation and complete responses
        self.llm_client = UnifiedLLMClient(
            azure_api_key=azure_api_key,
            azure_endpoint=azure_endpoint,
            azure_deployment=azure_deployment,
            default_model=model_name,
            max_retries=3,
            retry_delay=2.0,
            timeout=300,
            api_version=api_version,
            enable_continuation=True  # Enable multi-round concatenation for complete responses
        )
        
        # ç¡®å®šæ¨¡åž‹çš„contexté™åˆ¶
        model_type = self.llm_client._detect_model_type(model_name)
        self.context_limit = self.llm_client.model_limits.context_window
        
        # Memory/context tracking
        self.conversation_history = []
        
        # ðŸ†• å…¨å±€å¯¹è¯åŽ†å²ç®¡ç† (è·¨Taskè¿žç»­è®°å¿†)
        self.global_conversation_history = []
    
    def _estimate_context_size(self, base_prompt: str, event_content: str, files: List[TaskFile]) -> int:
        """ä¼°ç®—å½“å‰contextå¤§å° (åŒ…å«å…¨å±€åŽ†å²)"""
        # ç²—ç•¥ä¼°ç®—ï¼š4ä¸ªå­—ç¬¦â‰ˆ1ä¸ªtoken
        size = len(self.system_prompt) // 4  # ç³»ç»Ÿæç¤ºè¯
        size += len(base_prompt) // 4
        
        if event_content and event_content != 'Normal business conditions':
            size += len(event_content) // 4
        
        # è®¡ç®—å…¨å±€å¯¹è¯åŽ†å²å¤§å°
        for interaction in self.global_conversation_history:
            size += len(interaction['agent_response']) // 4
            size += len(interaction['manager_feedback']) // 4
        
        # è®¡ç®—æ–‡ä»¶å†…å®¹å¤§å°
        if files:
            for file_obj in files:
                size += len(file_obj.content) // 4
        
        # åŠ ä¸Šæ ¼å¼åŒ–å­—ç¬¦çš„ä¼°ç®—
        size += 1000  # æ ¼å¼åŒ–å­—ç¬¦å’Œå…¶ä»–overhead
        
        return size
    
    def _truncate_global_history(self, max_history_tokens: int = 30000):
        """æ™ºèƒ½æˆªæ–­å…¨å±€å¯¹è¯åŽ†å²ï¼Œä¿ç•™æœ€è¿‘çš„äº¤äº’"""
        if not self.global_conversation_history:
            return
        
        # è®¡ç®—å½“å‰åŽ†å²å¤§å°
        total_tokens = 0
        for interaction in self.global_conversation_history:
            total_tokens += len(interaction['agent_response']) // 4
            total_tokens += len(interaction['manager_feedback']) // 4
        
        # å¦‚æžœè¶…è¿‡é™åˆ¶ï¼Œä»Žæœ€æ—©çš„å¼€å§‹åˆ é™¤
        while total_tokens > max_history_tokens and len(self.global_conversation_history) > 1:
            # ç§»é™¤æœ€æ—©çš„äº¤äº’
            removed = self.global_conversation_history.pop(0)
            total_tokens -= len(removed['agent_response']) // 4
            total_tokens -= len(removed['manager_feedback']) // 4
            print(f"âš ï¸  [AGENT] Truncated history: removed {removed['task_id']} Round {removed['round']}")
    
    def add_interaction_to_global_history(self, task_id: str, round_num: int, agent_response: str, manager_feedback: str, task_complete: bool):
        """æ·»åŠ äº¤äº’åˆ°å…¨å±€åŽ†å²"""
        interaction_record = {
            'task_id': task_id,
            'round': round_num,
            'agent_response': agent_response,
            'manager_feedback': manager_feedback,
            'task_complete': task_complete
        }
        self.global_conversation_history.append(interaction_record)
    
    def _build_enhanced_prompt_with_history(self, task: Task, event_content: str) -> str:
        """æž„å»ºåŒ…å«å…¨å±€åŽ†å²çš„å®Œæ•´prompt"""
        # 1. æ™ºèƒ½æˆªæ–­åŽ†å²ä»¥é¿å…contextæº¢å‡º
        estimated_size = self._estimate_context_size(task.base_prompt, event_content, task.files)
        print(f"ðŸ“Š [AGENT] Estimated context size: ~{estimated_size} tokens")
        
        # åŠ¨æ€è®¡ç®—context limité˜ˆå€¼ï¼ˆä¿ç•™20%ä½™é‡ï¼‰
        context_threshold = int(self.context_limit * 0.8)
        if estimated_size > context_threshold:
            print(f"âš ï¸  [AGENT] Context approaching limit ({estimated_size}/{context_threshold}), truncating history...")
            self._truncate_global_history(max_history_tokens=30000)
            estimated_size_after = self._estimate_context_size(task.base_prompt, event_content, task.files)
            print(f"âœ… [AGENT] Context size after truncation: ~{estimated_size_after} tokens")
        
        # 2. æž„å»ºåŸºç¡€prompt
        if event_content and event_content != 'Normal business conditions':
            enhanced_prompt = f"""{task.base_prompt}

=== CURRENT SITUATION ===
{event_content}"""
        else:
            enhanced_prompt = task.base_prompt
        
        # 3. æ·»åŠ å…¨å±€å¯¹è¯åŽ†å² (å¦‚æžœæœ‰)
        if self.global_conversation_history:
            conversation_context = []
            for interaction in self.global_conversation_history:
                task_info = interaction.get('task_id', 'Unknown')
                round_info = interaction.get('round', 1)
                conversation_context.append(f"[{task_info} - Round {round_info}]")
                conversation_context.append(f"Assistant: {interaction['agent_response']}")
                conversation_context.append("")
                conversation_context.append(f"Manager: {interaction['manager_feedback']}")
                conversation_context.append("")
            
            enhanced_prompt += f"""

=== CONVERSATION HISTORY ===
{chr(10).join(conversation_context)}

[Current Task: {task.task_id}]
Please continue based on the conversation history above."""
        else:
            enhanced_prompt += f"""

[Current Task: {task.task_id}]
Please provide a comprehensive response to complete this task."""
        
        return enhanced_prompt

    def process_task_with_event(self, task: Task, event_content: str) -> Dict[str, Any]:
        """
        å¤„ç†å¸¦æœ‰äº‹ä»¶çš„ä»»åŠ¡ - è‡ªåŠ¨ç®¡ç†contextå’ŒåŽ†å²
        
        Args:
            task: Task object with prompt and files
            event_content: Event content (e.g., pressure situation)
            
        Returns:
            Dictionary containing agent response and metadata
        """
        try:
            # æž„å»ºåŒ…å«å…¨å±€åŽ†å²çš„enhanced prompt
            enhanced_prompt = self._build_enhanced_prompt_with_history(task, event_content)
            
            # ä½¿ç”¨enhanced promptæ–¹æ³•å¤„ç†ä»»åŠ¡
            return self.process_task_with_enhanced_prompt(task, enhanced_prompt, manager_feedback_history=None)
            
        except Exception as e:
            print(f"âŒ [AGENT] Error in task processing with event: {e}")
            raise

    def process_task(self, 
                    task: Task, 
                    manager_feedback_history: Optional[List[str]]) -> Dict[str, Any]:
        """
        Process a task with file attachments using unified LLM client with retry mechanism
        
        Args:
            task: Task object with prompt and files
            manager_feedback_history: Previous feedback from manager
            
        Returns:
            Dictionary containing agent response and metadata
        """
        
        try:
            # ðŸ†• ä½¿ç”¨é‡è¯•æœºåˆ¶è¿›è¡Œä»»åŠ¡å¤„ç†
            result = self.retry_handler.retry_with_warnings(
                self._single_task_attempt,
                "AGENT",
                "task processing",
                task, manager_feedback_history
            )
            return result
                
        except Exception as e:
            error_msg = f"Agent processing failed after retries: {str(e)}"
            # çŽ°åœ¨è¿™æ˜¯çœŸæ­£çš„å¤±è´¥ï¼Œå¯ä»¥è¿”å›žé”™è¯¯
            return {
                'response': f"I apologize, but I encountered an error processing this task: {error_msg}",
                'prompt_used': "",
                'files_processed': 0,
                'success': False,
                'error': error_msg,
                'llm_metadata': None
            }
    
    def _single_task_attempt(self, task: Task, manager_feedback_history: Optional[List[str]]) -> Dict[str, Any]:
        """å•æ¬¡ä»»åŠ¡å¤„ç†å°è¯• - è¢«é‡è¯•æœºåˆ¶è°ƒç”¨"""
        
        # 1. èŽ·å–ä»»åŠ¡prompt - ä½¿ç”¨æ­£ç¡®çš„Taskç»“æž„
        base_prompt = task.base_prompt
        
        # 2. Build file context
        file_context = self._build_file_context(task.files)
        
        # 3. Build full prompt
        full_prompt = self._build_full_prompt(base_prompt, file_context)
        
        # 4. Add manager feedback context if available
        if manager_feedback_history:
            feedback_context = self._build_feedback_context(manager_feedback_history)
            full_prompt = feedback_context + "\n\n" + full_prompt
        
        # 5. Prepare messages for LLM client
        messages = [
            {"role": "user", "content": full_prompt}
        ]
        
        # 5.5. Check context size before LLM call
        estimated_tokens = self.llm_client.estimate_tokens(self.system_prompt + full_prompt)
        
        if estimated_tokens > self.context_limit:
            # Contextæº¢å‡º - è®°å½•è­¦å‘Šä½†ä¸æŠ›å‡ºé”™è¯¯ï¼ŒAgentåº”è¯¥èƒ½å¤„ç†é•¿å¯¹è¯
            print(f"âš ï¸  [AGENT WARNING] Context approaching limit: {estimated_tokens}/{self.context_limit} tokens")
            print(f"âš ï¸  Full prompt length: {len(full_prompt)} chars")
            print(f"âš ï¸  Agent will proceed with current context, but may need history truncation in future rounds")
        
        # 6. Call unified LLM client with complete response requirement
        llm_result = self.llm_client.complete_chat(
            messages=messages,
            model=self.model_name,
            max_tokens=self.max_tokens,
            temperature=None,  # ä½¿ç”¨OpenAIé»˜è®¤å€¼
            system_role=self.system_prompt,
            require_complete_response=True  # Enable multi-round concatenation for complete responses
        )
        
        if not llm_result['success']:
            # æŠ›å‡ºå¼‚å¸¸è®©é‡è¯•æœºåˆ¶å¤„ç†
            raise RuntimeError(f"LLM call failed: {llm_result['error']}")
        
        agent_response = llm_result['content']
        
        # 7. Store conversation history - å®Œæ•´ä¿å­˜ï¼Œä¸æˆªæ–­
        self.conversation_history.append({
            'day': task.day,
            'task_id': task.task_id,
            'prompt_full': full_prompt,           # å®Œæ•´prompt
            'response_full': agent_response,      # å®Œæ•´å›žå¤
            'files_provided': len(task.files) if task.files else 0,
            'llm_rounds': llm_result['total_rounds'],
            'tokens_used': llm_result['tokens_used']
        })
        
        return {
            'response': agent_response,
            'prompt_used': full_prompt,
            'files_processed': len(task.files) if task.files else 0,
            'success': True,
            'error': None,
            'llm_metadata': {
                'is_complete': llm_result['is_complete'],
                'is_truncated': llm_result['is_truncated'],
                'total_rounds': llm_result['total_rounds'],
                'tokens_used': llm_result['tokens_used'],
                'finish_reason': llm_result['finish_reason']
            }
        }
    
    def process_task_with_enhanced_prompt(self,
                                        task: Task,
                                        enhanced_prompt: str,
                                        manager_feedback_history: Optional[List[str]]) -> Dict[str, Any]:
        """
        Process a task with a pre-enhanced prompt (using Event System)
        
        Args:
            task: Task object with files and metadata
            enhanced_prompt: Already enhanced prompt (base_prompt + event if applicable)
            manager_feedback_history: Previous feedback from manager
            
        Returns:
            Dictionary containing agent response and metadata
        """
        
        # 1. Build file context
        file_context = self._build_file_context(task.files)
        
        # 2. Build full prompt with file context
        full_prompt = self._build_full_prompt(enhanced_prompt, file_context)
        
        # 3. Add manager feedback context if available
        if manager_feedback_history:
            feedback_context = self._build_feedback_context(manager_feedback_history)
            full_prompt = feedback_context + "\n\n" + full_prompt
        
        # 4. Prepare messages for LLM client
        messages = [
            {"role": "user", "content": full_prompt}
        ]
        
        # 4.5. Check context size before LLM call
        estimated_tokens = self.llm_client.estimate_tokens(self.system_prompt + full_prompt)
        
        if estimated_tokens > self.context_limit:
            # Contextæº¢å‡º - è®°å½•è­¦å‘Šä½†ä¸æŠ›å‡ºé”™è¯¯ï¼ŒAgentåº”è¯¥èƒ½å¤„ç†é•¿å¯¹è¯
            print(f"âš ï¸  [AGENT WARNING] Context approaching limit: {estimated_tokens}/{self.context_limit} tokens")
            print(f"âš ï¸  Enhanced prompt length: {len(full_prompt)} chars")
            print(f"âš ï¸  Agent will proceed with current context, but may need history truncation in future rounds")
        
        # 5. Call unified LLM client with complete response requirement
        llm_result = self.llm_client.complete_chat(
            messages=messages,
            model=self.model_name,
            max_tokens=self.max_tokens,
            temperature=None,  # ä½¿ç”¨OpenAIé»˜è®¤å€¼
            system_role=self.system_prompt,
            require_complete_response=True
        )
            
        if not llm_result['success']:
            # æŠ›å‡ºå¼‚å¸¸è®©é‡è¯•æœºåˆ¶å¤„ç†
            raise RuntimeError(f"LLM call failed: {llm_result['error']}")
        
        agent_response = llm_result['content']
        
        # 6. Store conversation history - å®Œæ•´ä¿å­˜ï¼Œä¸æˆªæ–­
        self.conversation_history.append({
            'day': task.day,
            'task_id': task.task_id,
            'prompt_full': full_prompt,           # å®Œæ•´prompt
            'response_full': agent_response,      # å®Œæ•´å›žå¤
            'files_provided': len(task.files) if task.files else 0,
            'llm_rounds': llm_result['total_rounds'],
            'tokens_used': llm_result['tokens_used']
        })
        
        return {
            'response': agent_response,
            'prompt_used': full_prompt,
            'files_processed': len(task.files) if task.files else 0,
            'success': True,
            'error': None,
            'llm_metadata': {
                'is_complete': llm_result['is_complete'],
                'is_truncated': llm_result['is_truncated'],
                'total_rounds': llm_result['total_rounds'],
                'tokens_used': llm_result['tokens_used'],
                'finish_reason': llm_result['finish_reason']
            }
        }
            

    
    def _build_file_context(self, files: Optional[List[TaskFile]]) -> str:
        """Build file context section for the prompt"""
        if not files:
            return ""
        
        file_context = "\n\n=== ATTACHED FILES FOR ANALYSIS ===\n"
        
        for file_obj in files:
            file_context += f"\n--- FILE: {file_obj.filename} ---\n"
            if file_obj.description:
                file_context += f"Description: {file_obj.description}\n"
            file_context += f"Type: {file_obj.file_type}\n"
            file_context += f"Content:\n{file_obj.content}\n"
        
        file_context += "\n=== END OF ATTACHED FILES ===\n"
        return file_context
    
    def _build_full_prompt(self, base_prompt: str, file_context: str) -> str:
        """Combine base prompt with file context"""
        if file_context:
            return f"{base_prompt}{file_context}\n\nPlease analyze the attached files and provide your complete response based on the task requirements above. Take your time to provide a thorough and comprehensive analysis."
        else:
            return base_prompt
    
    def _build_feedback_context(self, feedback_history: List[str]) -> str:
        """Build context from follow-up requests (legacy manager feedback removed)"""
        if not feedback_history:
            return ""
        
        context = "=== FOLLOW-UP REQUESTS ===\n"
        for i, followup in enumerate(feedback_history, 1):
            context += f"Request {i}: {followup}\n"
        context += "=== END REQUESTS ===\n"
        context += "Please address these follow-up requests in your current response."
        
        return context
    
    def get_conversation_summary(self) -> Dict[str, Any]:
        """Get summary of agent's conversation history"""
        total_tokens = sum(h.get('tokens_used', 0) for h in self.conversation_history)
        total_rounds = sum(h.get('llm_rounds', 1) for h in self.conversation_history)
        
        return {
            'total_tasks_processed': len(self.conversation_history),
            'tasks_with_files': sum(1 for h in self.conversation_history if h['files_provided'] > 0),

            'llm_statistics': {
                'total_tokens_used': total_tokens,
                'total_llm_rounds': total_rounds,
                'average_rounds_per_task': total_rounds / max(len(self.conversation_history), 1)
            },
            'recent_tasks': self.conversation_history[-5:] if self.conversation_history else []
        }
    
    def get_llm_statistics(self) -> Dict[str, Any]:
        """Get LLM client statistics"""
        return self.llm_client.get_statistics()
    
    def reset_conversation(self):
        """Reset conversation history"""
        self.conversation_history = [] 