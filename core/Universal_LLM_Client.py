# core/Universal_LLM_Client.py
# ç»Ÿä¸€çš„LLMå®¢æˆ·ç«¯ - æ”¯æŒOpenAIå’ŒAzureçš„ç»Ÿä¸€æ¥å£

import os
import time
import re
from typing import Dict, Any, List, Optional
from openai import OpenAI
import openai
try:
    from tenacity import (
        retry,
        stop_after_attempt,
        wait_random_exponential,
        retry_if_exception_type
    )
    TENACITY_AVAILABLE = True
except ImportError:
    TENACITY_AVAILABLE = False
    print("âš ï¸  Warning: tenacity library not installed. Rate limit handling disabled.")
    print("   Install with: pip install tenacity")

class UniversalLLMClient:
    """
    ç»Ÿä¸€çš„LLMå®¢æˆ·ç«¯ - æ ¹æ®é…ç½®é€‰æ‹©OpenAIæˆ–Azure
    ä¿æŒä¸Unified_LLM_Handlerç›¸åŒçš„æ¥å£
    """
    
    def __init__(self, provider: str, config: Dict[str, Any]):
        """
        åˆå§‹åŒ–ç»Ÿä¸€å®¢æˆ·ç«¯
        
        Args:
            provider: "openai" æˆ– "azure"
            config: æä¾›å•†ç‰¹å®šçš„é…ç½®
        """
        self.provider = provider.lower()
        self.config = config
        
        if self.provider == "openai":
            self._init_openai()
        elif self.provider == "azure":
            self._init_azure()
        elif self.provider == "openrouter":
            self._init_openrouter()
        else:
            raise ValueError(f"Unsupported provider: {provider}. Supported: openai, azure, openrouter")
    
    def _init_openai(self):
        """åˆå§‹åŒ–OpenAIå®¢æˆ·ç«¯"""
        from pathlib import Path
        
        # æ”¯æŒä¸¤ç§é…ç½®æ–¹å¼ï¼šç›´æ¥api_keyæˆ–api_key_envç¯å¢ƒå˜é‡å
        if 'api_key' in self.config:
            api_key = self.config['api_key']  # ç›´æ¥ä»é…ç½®ä¸­è·å–ï¼ˆé€šå¸¸å·²ç»æ˜¯ç¯å¢ƒå˜é‡å€¼ï¼‰
        elif 'api_key_env' in self.config:
            api_key_env = self.config['api_key_env']
            api_key = os.getenv(api_key_env)
        else:
            raise ValueError("OpenAI configuration must include either 'api_key' or 'api_key_env'")
        
        # å¦‚æœç¯å¢ƒå˜é‡ä¸­æ²¡æœ‰ï¼Œå°è¯•ä»é¡¹ç›®æ ¹ç›®å½•çš„.envæ–‡ä»¶è¯»å–
        if not api_key:
            # æ‰¾åˆ°é¡¹ç›®æ ¹ç›®å½•
            current_file = Path(__file__)
            if current_file.parent.name == 'core':
                project_root = current_file.parent.parent
            else:
                project_root = Path.cwd()
            
            env_file = project_root / '.env'
            if env_file.exists():
                # æ‰‹åŠ¨è§£æ.envæ–‡ä»¶
                with open(env_file, 'r') as f:
                    for line in f:
                        line = line.strip()
                        if line and not line.startswith('#'):
                            if '=' in line:
                                key, value = line.split('=', 1)
                                key = key.strip()
                                value = value.strip().strip('"').strip("'")
                                if key == api_key_env:
                                    api_key = value
                                    break
        
        if not api_key:
            raise ValueError(f"Environment variable {api_key_env} not found in environment or .env file")
        
        self.client = OpenAI(api_key=api_key)
        self.model = self.config.get('model_name', self.config.get('model', 'unknown'))
        
        # æ¨¡æ‹ŸUnified_LLM_Handlerçš„å±æ€§
        self.model_limits = type('obj', (object,), {
            'context_window': 128000,  # GPT-4o context limit
            'max_output_tokens': 4096
        })()
        
        print(f"[UNIVERSAL_LLM] Initialized OpenAI client with model: {self.model}")
        
        # æ·»åŠ rate limitå¤„ç†
        if TENACITY_AVAILABLE:
            self._setup_rate_limit_handling()
        
    def _setup_rate_limit_handling(self):
        """è®¾ç½®OpenAI rate limitå¤„ç†æœºåˆ¶"""
        
        # ä¿å­˜åŸå§‹æ–¹æ³•çš„å¼•ç”¨
        self._original_chat_create = self.client.chat.completions.create
        
        # åˆ›å»ºå¸¦é‡è¯•çš„åŒ…è£…æ–¹æ³•
        @retry(
            retry=retry_if_exception_type(openai.RateLimitError),
            wait=wait_random_exponential(min=1, max=60),
            stop=stop_after_attempt(6),
            before_sleep=self._rate_limit_warning
        )
        def _chat_completion_with_backoff(**kwargs):
            # è°ƒç”¨åŸå§‹æ–¹æ³•ï¼Œé¿å…é€’å½’
            return self._original_chat_create(**kwargs)
        
        # æ›¿æ¢æ–¹æ³•
        self.client.chat.completions.create = _chat_completion_with_backoff
    
    def _rate_limit_warning(self, retry_state):
        """Rate limitè§¦å‘æ—¶çš„è­¦å‘Šå›è°ƒ"""
        attempt = retry_state.attempt_number
        exception = retry_state.outcome.exception()
        
        if isinstance(exception, openai.RateLimitError):
            wait_time = retry_state.next_action.sleep
            print(f"ğŸš¨ [RATE LIMIT WARNING] OpenAI rate limit hit!")
            print(f"   ğŸ“Š Attempt: {attempt}/6")
            print(f"   â±ï¸  Waiting: {wait_time:.1f} seconds before retry")
            print(f"   ğŸ’¡ Consider upgrading your OpenAI tier or reducing request frequency")
            
            # å¦‚æœè¿ç»­å¤šæ¬¡è§¦å‘ï¼Œç»™å‡ºæ›´å¼ºçƒˆçš„è­¦å‘Š
            if attempt >= 3:
                print(f"âš ï¸  [CRITICAL] Multiple rate limit hits detected!")
                print(f"   ğŸ”§ Suggestions:")
                print(f"      - Check your OpenAI usage tier and limits")
                print(f"      - Reduce experiment frequency")
                print(f"      - Consider using Azure instead of OpenAI")
    
    def _log_rate_limit_headers(self, response):
        """ä»OpenAIå“åº”ä¸­æå–å¹¶æ˜¾ç¤ºrate limitä¿¡æ¯"""
        if hasattr(response, 'headers') and self.provider == 'openai':
            headers = response.headers
            
            # æå–rate limitä¿¡æ¯
            limit_requests = headers.get('x-ratelimit-limit-requests')
            remaining_requests = headers.get('x-ratelimit-remaining-requests')
            limit_tokens = headers.get('x-ratelimit-limit-tokens')
            remaining_tokens = headers.get('x-ratelimit-remaining-tokens')
            reset_requests = headers.get('x-ratelimit-reset-requests')
            reset_tokens = headers.get('x-ratelimit-reset-tokens')
            
            if limit_requests and remaining_requests:
                requests_usage = (int(limit_requests) - int(remaining_requests)) / int(limit_requests) * 100
                if requests_usage > 80:  # è¶…è¿‡80%ä½¿ç”¨ç‡æ—¶è­¦å‘Š
                    print(f"âš ï¸  [RATE LIMIT] Request usage: {requests_usage:.1f}% ({remaining_requests}/{limit_requests} remaining)")
                    if reset_requests:
                        print(f"   ğŸ”„ Resets in: {reset_requests}")
            
            if limit_tokens and remaining_tokens:
                tokens_usage = (int(limit_tokens) - int(remaining_tokens)) / int(limit_tokens) * 100
                if tokens_usage > 80:  # è¶…è¿‡80%ä½¿ç”¨ç‡æ—¶è­¦å‘Š  
                    print(f"âš ï¸  [RATE LIMIT] Token usage: {tokens_usage:.1f}% ({remaining_tokens}/{limit_tokens} remaining)")
                    if reset_tokens:
                        print(f"   ğŸ”„ Resets in: {reset_tokens}")
    
    def _init_azure(self):
        """ç›´æ¥åˆå§‹åŒ–Azureå®¢æˆ·ç«¯ - ä½¿ç”¨é…ç½®å‚æ•°"""
        from openai import AzureOpenAI
        
        # ä»é…ç½®ä¸­è·å–Azureå‚æ•°
        api_key = self.config['azure_api_key']
        endpoint = self.config['azure_endpoint']
        
        # æ­£ç¡®åˆå§‹åŒ–Azure OpenAIå®¢æˆ·ç«¯
        self.client = AzureOpenAI(
            api_key=api_key,
            azure_endpoint=endpoint,
            api_version=self.config['azure_api_version']
        )
        
        self.model = self.config['model_name']
        self.azure_deployment = self.config['azure_deployment']
        
        # è®¾ç½®æ¨¡å‹é™åˆ¶ - é»˜è®¤GPT-4oè§„æ ¼
        from collections import namedtuple
        ModelLimits = namedtuple('ModelLimits', ['context_window', 'max_output_tokens'])
        self.model_limits = ModelLimits(128000, 4096)
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_calls': 0,
            'successful_calls': 0,
            'total_tokens_used': 0
        }
        
        print(f"[UNIVERSAL_LLM] Initialized Azure client with model: {self.model}")
    
    def _init_openrouter(self):
        """åˆå§‹åŒ–OpenRouterå®¢æˆ·ç«¯"""
        from pathlib import Path
        import requests
        
        # æ”¯æŒç›´æ¥çš„api_keyé…ç½®æˆ–ç¯å¢ƒå˜é‡å¼•ç”¨
        if 'api_key' in self.config:
            # ç›´æ¥ä»é…ç½®ä¸­è·å–API key
            api_key = self.config['api_key']
        elif 'api_key_env' in self.config:
            # ä»ç¯å¢ƒå˜é‡è·å–API keyï¼ˆæ—§æ ¼å¼ï¼‰
            api_key_env = self.config['api_key_env']
            api_key = os.getenv(api_key_env)
            
            # å¦‚æœç¯å¢ƒå˜é‡ä¸­æ²¡æœ‰ï¼Œå°è¯•ä»é¡¹ç›®æ ¹ç›®å½•çš„.envæ–‡ä»¶è¯»å–
            if not api_key:
                # æ‰¾åˆ°é¡¹ç›®æ ¹ç›®å½•
                current_file = Path(__file__)
                if current_file.parent.name == 'core':
                    project_root = current_file.parent.parent
                else:
                    project_root = Path.cwd()
                
                env_file = project_root / '.env'
                if env_file.exists():
                    with open(env_file, 'r') as f:
                        for line in f:
                            line = line.strip()
                            if line.startswith(f'{api_key_env}='):
                                api_key = line.split('=', 1)[1].strip().strip('"').strip("'")
                                break
        else:
            raise ValueError("Neither 'api_key' nor 'api_key_env' found in OpenRouter config")
        
        if not api_key:
            if 'api_key_env' in self.config:
                raise ValueError(f"Environment variable {self.config['api_key_env']} not found in environment or .env file")
            else:
                raise ValueError("OpenRouter API key not found in configuration")
        
        # OpenRouterä½¿ç”¨requestsï¼Œä¸æ˜¯OpenAIå®¢æˆ·ç«¯
        self.api_key = api_key
        self.api_base = "https://openrouter.ai/api/v1"
        self.model = self.config.get('model_name', self.config.get('model', 'unknown'))
        
        # è®¾ç½®æ¨¡å‹é™åˆ¶ - Gemini-2.5-Proè§„æ ¼
        from collections import namedtuple
        ModelLimits = namedtuple('ModelLimits', ['context_window', 'max_output_tokens'])
        self.model_limits = ModelLimits(1048576, 32768)  # Gemini-2.5-Proçš„è§„æ ¼
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_calls': 0,
            'successful_calls': 0,
            'total_tokens_used': 0
        }
        
        print(f"[UNIVERSAL_LLM] Initialized OpenRouter client with model: {self.model}")
    
    def complete_chat(self, messages: List[Dict[str, str]], model: Optional[str] = None, 
                     max_tokens: Optional[int] = None, temperature: Optional[float] = None,
                     system_role: Optional[str] = None, require_complete_response: bool = False,
                     caller: str = "UNKNOWN") -> Dict[str, Any]:
        """
        ç»Ÿä¸€çš„èŠå¤©è¡¥å…¨æ¥å£
        è¿”å›æ ¼å¼ä¸Unified_LLM_Handlerä¿æŒä¸€è‡´
        """
        if self.provider == "openai":
            return self._openai_complete_chat(messages, model, max_tokens, temperature, system_role, require_complete_response, caller)
        elif self.provider == "azure":
            return self._azure_complete_chat(messages, model, max_tokens, temperature, system_role, require_complete_response, caller)
        elif self.provider == "openrouter":
            return self._openrouter_complete_chat(messages, model, max_tokens, temperature, system_role, require_complete_response, caller)
        else:
            return {'success': False, 'error': f'Unsupported provider: {self.provider}'}
    
    def _truncate_messages_by_task(self, messages: List[Dict[str, str]], attempt: int) -> List[Dict[str, str]]:
        """å¹³æ–¹å¢é•¿taskåˆ é™¤ï¼šç¬¬1æ¬¡åˆ 1ä¸ªtaskï¼Œç¬¬2æ¬¡åˆ 2ä¸ªtaskï¼Œç¬¬3æ¬¡åˆ 3ä¸ªtaskï¼Œç¬¬4æ¬¡åˆ 4ä¸ªtaskï¼Œç„¶åerror"""
        if len(messages) == 0:
            raise RuntimeError("Cannot truncate empty message list")
        
        truncated = messages.copy()
        original_count = len(messages)
        
        # æœ€å¤šå°è¯•4æ¬¡ï¼Œå¹³æ–¹å¢é•¿åˆ é™¤
        max_attempts = 4
        if attempt > max_attempts:
            raise RuntimeError(f"Context overflow persists after {max_attempts} attempts with quadratic task removal, cannot proceed")
        
        # æ›´æ¿€è¿›çš„ç´¯ç§¯åˆ é™¤ï¼šattempt 1åˆ 2ä¸ªtaskï¼Œattempt 2å†åˆ 4ä¸ªtaskï¼Œattempt 3å†åˆ 6ä¸ªtaskï¼Œattempt 4å†åˆ 8ä¸ªtask
        tasks_to_remove = attempt * 2
        
        print(f"[UNIVERSAL_LLM] Attempt {attempt}/{max_attempts}: Removing {tasks_to_remove} complete tasks (quadratic progression)")
        
        # åˆ é™¤æŒ‡å®šæ•°é‡çš„å®Œæ•´tasks
        removed_task_count = 0
        total_removed_messages = 0
        
        for task_num in range(tasks_to_remove):
            # æŸ¥æ‰¾æœ€æ—©çš„taskå¼€å¤´å¹¶åˆ é™¤è¯¥taskçš„æ‰€æœ‰æ¶ˆæ¯
            task_removed = False
            target_task_id = None
            
            # ä»å¤´å¼€å§‹æ‰¾taskæ ‡è®°ï¼šå¦‚ "[TASK_01 Round 1]"
            i = 0
            while i < len(truncated):
                msg_content = truncated[i]['content']
                
                # æ‰¾åˆ°taskå¼€å¤´æ ‡è®° - åŒ¹é… [XXX Round 1] æ ¼å¼
                if 'Round 1]' in msg_content and msg_content.startswith('['):
                    # æå–task ID - æ ¼å¼: [MARKET-SIZE-ANALYSIS Round 1]
                    task_match = re.search(r'\[([A-Z\-]+)\s+Round\s+1\]', msg_content)
                    if task_match:
                        target_task_id = task_match.group(1)
                        print(f"[UNIVERSAL_LLM] Removing task {task_num + 1}/{tasks_to_remove}: {target_task_id}")
                        
                        # åˆ é™¤è¯¥taskçš„æ‰€æœ‰æ¶ˆæ¯
                        task_message_count = 0
                        while i < len(truncated):
                            current_msg = truncated[i]['content']
                            
                            # æ£€æŸ¥æ˜¯å¦è¿˜æ˜¯åŒä¸€ä¸ªtask - åŒ¹é… [XXX Round N] æ ¼å¼
                            current_task_match = re.search(r'\[([A-Z\-]+)\s+Round\s+\d+\]', current_msg)
                            if current_task_match:
                                current_task_id = current_task_match.group(1)
                                # å¦‚æœæ˜¯ä¸åŒçš„taskï¼Œåœæ­¢åˆ é™¤
                                if current_task_id != target_task_id:
                                    break
                            
                            # åˆ é™¤å½“å‰æ¶ˆæ¯ï¼ˆå±äºtarget taskæˆ–è€…æ˜¯response/feedbackï¼‰
                            truncated.pop(i)
                            task_message_count += 1
                            total_removed_messages += 1
                            # iä¸éœ€è¦+1ï¼Œå› ä¸ºpopåä¸‹ä¸€ä¸ªå…ƒç´ ä¼šç§»åˆ°å½“å‰ä½ç½®
                        
                        print(f"[UNIVERSAL_LLM] Task {target_task_id}: removed {task_message_count} messages")
                        task_removed = True
                        removed_task_count += 1
                        break
                else:
                    i += 1
            
            if not task_removed:
                # FAIL-FAST: å¦‚æœæ‰¾ä¸åˆ°taskæ ‡è®°ï¼Œè¯´æ˜æ•°æ®æ ¼å¼é”™è¯¯ï¼Œç«‹å³æŠ¥é”™
                raise RuntimeError(f"Unable to find task marker for removal attempt {task_num + 1}/{tasks_to_remove}. "
                                 f"Message format may be corrupted or incompatible. "
                                 f"Current messages: {len(truncated)}, Expected task format: [TASK-ID Round 1]")
        
        print(f"[UNIVERSAL_LLM] Summary: removed {removed_task_count} complete tasks, {total_removed_messages} total messages")
        print(f"[UNIVERSAL_LLM] Original: {original_count} messages -> After truncation: {len(truncated)} messages")
        
        # æ˜¾ç¤ºå‰©ä½™contextçš„å‰50ä¸ªå­—ç¬¦
        if len(truncated) > 0:
            first_msg_preview = truncated[0]['content'][:50].replace('\n', ' ')
            print(f"[UNIVERSAL_LLM] Remaining context starts with: '{first_msg_preview}...'")
        
        return truncated
    
    def _openai_complete_chat(self, messages: List[Dict[str, str]], model: Optional[str] = None,
                            max_tokens: Optional[int] = None, temperature: Optional[float] = None,
                            system_role: Optional[str] = None, require_complete_response: bool = False,
                            caller: str = "UNKNOWN") -> Dict[str, Any]:
        """OpenAI APIè°ƒç”¨"""
        try:
            # å¤„ç†ç³»ç»Ÿæ¶ˆæ¯
            if system_role:
                messages = [{"role": "system", "content": system_role}] + messages
            
            # ç¡®å®šä½¿ç”¨çš„æ¨¡å‹
            used_model = model or self.model
            
            # å‚æ•°è®¾ç½®
            call_params = {
                "model": used_model,
                "messages": messages,
            }
            
            # æ ¹æ®æ¨¡å‹ç±»å‹è®¾ç½®tokenå‚æ•°
            if used_model.startswith('o4-') or used_model.startswith('o3-'):
                # o4-miniå’Œo3ç³»åˆ—ä½¿ç”¨max_completion_tokens
                call_params["max_completion_tokens"] = max_tokens or self.model_limits.max_output_tokens
            else:
                # å…¶ä»–æ¨¡å‹ä½¿ç”¨max_tokens
                call_params["max_tokens"] = max_tokens or self.model_limits.max_output_tokens
            
            if temperature is not None:
                call_params["temperature"] = temperature
            
            print(f"[{caller}] Using: {call_params['model']}")
            
            # APIè°ƒç”¨ (å·²ç»åŒ…å«rate limitå¤„ç†)
            response = self.client.chat.completions.create(**call_params)
            
            # ç›‘æ§rate limitçŠ¶æ€
            self._log_rate_limit_headers(response)
            
            # æ ¼å¼åŒ–è¿”å›ç»“æœï¼ŒåŒ¹é…Unified_LLM_Handleræ ¼å¼
            content = response.choices[0].message.content
            finish_reason = response.choices[0].finish_reason
            
            return {
                'success': True,
                'content': content,
                'finish_reason': finish_reason,
                'is_complete': finish_reason != 'length',
                'is_truncated': finish_reason == 'length',
                'total_rounds': 1,
                'model_used': response.model,
                'tokens_used': response.usage.total_tokens if response.usage else 0
            }
            
        except Exception as e:
            error_str = str(e)
            print(f"[UNIVERSAL_LLM] OpenAI API call failed: {error_str}")
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯context lengthé”™è¯¯
            context_error_info = self.parse_context_length_error(error_str)
            
            return {
                'success': False,
                'content': '',
                'finish_reason': 'error',
                'is_complete': False,
                'is_truncated': False,
                'total_rounds': 0,
                'model_used': model or self.model,
                'tokens_used': 0,
                'error': error_str,
                'context_overflow': context_error_info  # æ·»åŠ contextæº¢å‡ºä¿¡æ¯
            }
    
    def _azure_complete_chat(self, messages: List[Dict[str, str]], model: Optional[str] = None,
                           max_tokens: Optional[int] = None, temperature: Optional[float] = None,
                           system_role: Optional[str] = None, require_complete_response: bool = False,
                           caller: str = "UNKNOWN") -> Dict[str, Any]:
        """Azure APIè°ƒç”¨ - ç›´æ¥å®ç°"""
        try:
            self.stats['total_calls'] += 1
            
            # å¤„ç†ç³»ç»Ÿæ¶ˆæ¯
            if system_role:
                messages = [{"role": "system", "content": system_role}] + messages
            
            # ç¡®å®šä½¿ç”¨çš„æ¨¡å‹
            used_model = model or self.model
            
            # å‚æ•°è®¾ç½®
            call_params = {
                "model": self.azure_deployment,  # Azureä½¿ç”¨deploymentåç§°
                "messages": messages,
                "max_tokens": max_tokens or self.model_limits.max_output_tokens
            }
            
            if temperature is not None:
                call_params["temperature"] = temperature
            
            # ç®€åŒ–æ‰“å°ï¼Œåªæ˜¾ç¤ºæ¨¡å‹åå­—
            model_display = self.config.get('model_name', self.azure_deployment)
            print(f"[{caller}] Using: {model_display}")
            
            # APIè°ƒç”¨ - å¼ºåˆ¶ä½¿ç”¨æ­£ç¡®çš„APIç‰ˆæœ¬
            response = self.client.chat.completions.create(**call_params)
            
            # æ ¼å¼åŒ–è¿”å›ç»“æœ
            content = response.choices[0].message.content
            finish_reason = response.choices[0].finish_reason
            
            # ç»Ÿè®¡ä¿¡æ¯
            tokens_used = getattr(response.usage, 'total_tokens', 0) if hasattr(response, 'usage') else 0
            self.stats['total_tokens_used'] += tokens_used
            self.stats['successful_calls'] += 1
            
            return {
                'success': True,
                'content': content,
                'finish_reason': finish_reason,
                'is_complete': finish_reason != 'length',
                'is_truncated': finish_reason == 'length',
                'total_rounds': 1,
                'model_used': used_model,
                'tokens_used': tokens_used
            }
            
        except Exception as e:
            error_str = str(e)
            print(f"[UNIVERSAL_LLM] Azure API call failed: {error_str}")
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯context lengthé”™è¯¯
            context_error_info = self.parse_context_length_error(error_str)
            
            return {
                'success': False,
                'content': '',
                'finish_reason': 'error',
                'is_complete': False,
                'is_truncated': False,
                'total_rounds': 0,
                'model_used': model or self.model,
                'tokens_used': 0,
                'error': error_str,
                'context_overflow': context_error_info  # æ·»åŠ contextæº¢å‡ºä¿¡æ¯
            }
    
    def _openrouter_complete_chat(self, messages: List[Dict[str, str]], model: Optional[str] = None,
                                max_tokens: Optional[int] = None, temperature: Optional[float] = None,
                                system_role: Optional[str] = None, require_complete_response: bool = False,
                                caller: str = "UNKNOWN") -> Dict[str, Any]:
        """OpenRouter APIè°ƒç”¨ - ä½¿ç”¨requestså®ç°"""
        import requests
        import json
        import time
        
        try:
            self.stats['total_calls'] += 1
            
            # å¤„ç†ç³»ç»Ÿæ¶ˆæ¯
            if system_role:
                messages = [{"role": "system", "content": system_role}] + messages
            
            # ç¡®å®šä½¿ç”¨çš„æ¨¡å‹
            used_model = model or self.model
            
            # æ„å»ºè¯·æ±‚å‚æ•°
            payload = {
                "model": used_model,
                "messages": messages,
                "max_tokens": max_tokens or self.model_limits.max_output_tokens
            }
            
            if temperature is not None:
                payload["temperature"] = temperature
            
            # è¯·æ±‚å¤´
            headers = {
                "Authorization": f"Bearer {self.api_key}",
                "Content-Type": "application/json",
                "HTTP-Referer": "https://github.com/anthropics/claude-code",
                "X-Title": "DeceptioN Research Framework"
            }
            
            print(f"[{caller}] Using: {used_model}")
            
            # å‘é€è¯·æ±‚ - æ·»åŠ é‡è¯•æœºåˆ¶
            max_retries = 3
            response = None
            for attempt in range(max_retries):
                try:
                    response = requests.post(
                        f"{self.api_base}/chat/completions",
                        headers=headers,
                        json=payload,
                        timeout=120  # å¢åŠ è¶…æ—¶æ—¶é—´
                    )
                    # æˆåŠŸåæ·»åŠ å°å»¶è¿Ÿï¼Œé¿å…é€Ÿç‡é™åˆ¶
                    if attempt > 0:
                        print(f"[UNIVERSAL_LLM] OpenRouter request succeeded after {attempt + 1} attempts")
                    time.sleep(1)  # 1ç§’å»¶è¿Ÿé¿å…é€Ÿç‡é™åˆ¶
                    break
                except (requests.exceptions.ConnectionError, requests.exceptions.Timeout) as e:
                    if attempt < max_retries - 1:
                        wait_time = (attempt + 1) * 2  # 2, 4, 6ç§’é€’å¢ç­‰å¾…
                        print(f"[UNIVERSAL_LLM] OpenRouter connection failed (attempt {attempt + 1}/{max_retries}), retrying in {wait_time}s...")
                        print(f"  Error: {str(e)[:100]}")  # æ‰“å°é”™è¯¯ä¿¡æ¯å‰100å­—ç¬¦
                        time.sleep(wait_time)
                    else:
                        print(f"[UNIVERSAL_LLM] OpenRouter failed after {max_retries} attempts")
                        raise e
            
            if response.status_code == 200:
                try:
                    result = response.json()
                except json.JSONDecodeError as e:
                    # å“åº”ä¸å®Œæ•´æˆ–æ ¼å¼é”™è¯¯
                    print(f"[UNIVERSAL_LLM] OpenRouter response JSON decode error: {str(e)[:100]}")
                    print(f"[UNIVERSAL_LLM] Response text preview: {response.text[:200]}...")
                    return {
                        'success': False,
                        'content': '',
                        'finish_reason': 'error',
                        'is_complete': False,
                        'is_truncated': False,
                        'total_rounds': 0,
                        'model_used': used_model,
                        'tokens_used': 0,
                        'error': f"Response ended prematurely: {str(e)}"
                    }
                
                # æå–å“åº”å†…å®¹
                try:
                    content = result['choices'][0]['message']['content']
                    finish_reason = result['choices'][0]['finish_reason']
                except (KeyError, IndexError) as e:
                    print(f"[UNIVERSAL_LLM] OpenRouter response format error: {str(e)}")
                    print(f"[UNIVERSAL_LLM] Response structure: {result.keys() if isinstance(result, dict) else type(result)}")
                    return {
                        'success': False,
                        'content': '',
                        'finish_reason': 'error',
                        'is_complete': False,
                        'is_truncated': False,
                        'total_rounds': 0,
                        'model_used': used_model,
                        'tokens_used': 0,
                        'error': f"Invalid response format: {str(e)}"
                    }
                
                # ç»Ÿè®¡ä¿¡æ¯
                tokens_used = 0
                if 'usage' in result:
                    tokens_used = result['usage'].get('total_tokens', 0)
                    self.stats['total_tokens_used'] += tokens_used
                
                self.stats['successful_calls'] += 1
                
                return {
                    'success': True,
                    'content': content,
                    'finish_reason': finish_reason,
                    'is_complete': finish_reason != 'length',
                    'is_truncated': finish_reason == 'length',
                    'total_rounds': 1,
                    'model_used': used_model,
                    'tokens_used': tokens_used
                }
                
            else:
                error_msg = f"Status {response.status_code}: {response.text}"
                print(f"[UNIVERSAL_LLM] OpenRouter API call failed: {error_msg}")
                return {
                    'success': False,
                    'content': '',
                    'finish_reason': 'error',
                    'is_complete': False,
                    'is_truncated': False,
                    'total_rounds': 0,
                    'model_used': used_model,
                    'tokens_used': 0,
                    'error': error_msg
                }
                
        except Exception as e:
            print(f"[UNIVERSAL_LLM] OpenRouter API call failed: {e}")
            return {
                'success': False,
                'content': '',
                'finish_reason': 'error',
                'is_complete': False,
                'is_truncated': False,
                'total_rounds': 0,
                'model_used': model or self.model,
                'tokens_used': 0,
                'error': str(e)
            }
    
    def estimate_tokens(self, text: str) -> int:
        """ä¼°ç®—tokenæ•°é‡"""
        # ç»Ÿä¸€çš„ç®€å•ä¼°ç®—ï¼šå¤§çº¦4ä¸ªå­—ç¬¦=1ä¸ªtoken
        return len(text) // 4 + 1
    
    def get_statistics(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            'provider': self.provider,
            'model': self.model,
            **self.stats
        }
    
    def parse_context_length_error(self, error_msg: str) -> Optional[Dict[str, int]]:
        """
        è§£æcontext lengthè¶…é™é”™è¯¯ï¼Œæå–æœ€å¤§é™åˆ¶å’Œå®é™…è¯·æ±‚çš„tokenæ•°
        
        é”™è¯¯æ ¼å¼: 'This model's maximum context length is 128000 tokens. However, you requested 129947 tokens (113563 in the messages, 16384 in the completion)'
        
        Returns:
            Dict with:
            - 'max_context_length': æ¨¡å‹çš„æœ€å¤§contexté™åˆ¶
            - 'requested_tokens': å®é™…è¯·æ±‚çš„æ€»tokenæ•°
            - 'message_tokens': messagesä¸­çš„tokenæ•°
            - 'completion_tokens': completionä¸­çš„tokenæ•°
            None if not a context length error
        """
        # æ£€æŸ¥æ˜¯å¦æ˜¯context lengthé”™è¯¯
        if 'context_length_exceeded' not in error_msg and 'maximum context length' not in error_msg:
            return None
        
        result = {}
        
        # æå–æœ€å¤§contexté™åˆ¶
        max_match = re.search(r'maximum context length is (\d+) tokens', error_msg)
        if max_match:
            result['max_context_length'] = int(max_match.group(1))
        else:
            # å¦‚æœæ²¡æ‰¾åˆ°ï¼Œä½¿ç”¨é»˜è®¤å€¼
            result['max_context_length'] = 128000
        
        # æå–è¯·æ±‚çš„æ€»tokenæ•° - æ”¯æŒä¸¤ç§æ ¼å¼
        request_match = re.search(r'requested (\d+) tokens', error_msg)
        if not request_match:
            # å°è¯•å¦ä¸€ç§æ ¼å¼ï¼š"messages resulted in 129835 tokens"
            request_match = re.search(r'messages resulted in (\d+) tokens', error_msg)
            if not request_match:
                return None
        
        result['requested_tokens'] = int(request_match.group(1))
        
        # å°è¯•æå–messageså’Œcompletionçš„è¯¦ç»†ä¿¡æ¯
        detail_match = re.search(r'\((\d+) in the messages, (\d+) in the completion\)', error_msg)
        if detail_match:
            result['message_tokens'] = int(detail_match.group(1))
            result['completion_tokens'] = int(detail_match.group(2))
        else:
            # å¦‚æœæ²¡æœ‰è¯¦ç»†ä¿¡æ¯ï¼Œä¼°ç®—åˆ†é…
            # å¯¹äº"messages resulted in X tokens"æ ¼å¼ï¼Œå‡è®¾completionéœ€è¦16384
            result['message_tokens'] = result['requested_tokens'] - 16384
            result['completion_tokens'] = 16384
        
        # è®¡ç®—è¶…å‡ºäº†å¤šå°‘
        result['overflow_tokens'] = result['requested_tokens'] - result['max_context_length']
        
        return result