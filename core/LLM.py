# core/LLM.py
# DeceptioN LLM abstraction layer with unified LLM client

from typing import Dict, Any, Optional, List
from tasks.base import Task, TaskFile
from .Universal_LLM_Handler import UniversalLLMHandler
from utils.error_handler import PipelineErrorHandler, RetryHandler

class LLM:
    """
    DeceptioN LLM that can process tasks with file attachments
    Uses unified LLM client for robust token handling
    """
    
    def __init__(self, llm_provider: str, llm_config: Dict[str, Any], system_prompt: str, max_tokens: int):
        """
        Initialize the DeceptioN LLM
        
        Args:
            llm_provider: API provider (openai/azure)
            llm_config: Provider-specific configuration
            system_prompt: System prompt for the LLM
            max_tokens: Maximum tokens for responses
        """
        self.llm_provider = llm_provider
        self.llm_config = llm_config
        
        # ä¿æŒåŸæœ‰å±æ€§å…¼å®¹æ€§ - ä»configä¸­æå–æ¨¡å‹ä¿¡æ¯
        if llm_provider == 'openai':
            self.model_name = llm_config.get('model')
            if not self.model_name:
                raise ValueError("OpenAI config missing 'model' field")
            # ä¸ºå°†æ¥æ‰©å±•ä¿ç•™
            self.azure_api_key = None
            self.azure_endpoint = None
            self.azure_deployment = None
            self.api_version = None
        else:  # Azure
            # å°è¯•ä¸åŒçš„å­—æ®µå
            self.model_name = llm_config.get('model') or llm_config.get('model_name')
            if not self.model_name:
                raise ValueError("Azure config missing 'model' or 'model_name' field")
            self.azure_api_key = llm_config.get('azure_api_key')
            self.azure_endpoint = llm_config.get('azure_endpoint') 
            self.azure_deployment = llm_config.get('azure_deployment')
            self.api_version = llm_config.get('api_version')
        
        if system_prompt is None:
            raise ValueError("system_prompt is required and cannot be None")
        self.system_prompt = system_prompt
        
        self.max_tokens = max_tokens
        
        # ğŸ†• æ·»åŠ é‡è¯•å¤„ç†å™¨
        self.retry_handler = RetryHandler(max_retries=3)
        
        # Use universal LLM client - supports OpenAI and Azure
        self.llm_client = UniversalLLMHandler(llm_provider, llm_config)
        
        # è·å–æ¨¡å‹çš„contexté™åˆ¶
        self.context_limit = self.llm_client.model_limits.context_window
        
        # Memory/context tracking
        self.conversation_history = []
        
        # ğŸ†• å…¨å±€å¯¹è¯å†å²ç®¡ç† (è·¨Taskè¿ç»­è®°å¿†)
        self.global_conversation_history = []
        
        # å½“å‰ä»»åŠ¡å†…çš„å›å¤å†å² - ç”¨äºæ„å»ºChatGPTé£æ ¼æ¶ˆæ¯
        self.current_task_responses = []
        self.current_task_id = None
    
    def _estimate_context_size(self, base_prompt: str, event_content: str, files: List[TaskFile]) -> int:
        """ä¼°ç®—å½“å‰contextå¤§å° (åŒ…å«å…¨å±€å†å²)ï¼Œä½¿ç”¨tiktokenç²¾ç¡®è®¡ç®—"""
        
        # ä½¿ç”¨tiktokenç²¾ç¡®è®¡ç®—tokenæ•°ï¼ˆåªå¯¹GPTæ¨¡å‹ï¼‰+ æ¶ˆæ¯æ ¼å¼ä¿®æ­£
        def estimate_tokens_precise(text: str) -> int:
            if hasattr(self, 'llm_client') and hasattr(self.llm_client, 'provider'):
                if self.llm_client.provider in ['openai', 'azure', 'openrouter'] and 'gpt' in str(getattr(self.llm_client, 'model', '')).lower():
                    try:
                        import tiktoken
                        encoding = tiktoken.encoding_for_model("gpt-4o")
                        base_tokens = len(encoding.encode(text))
                        # åº”ç”¨æ¶ˆæ¯æ ¼å¼ä¿®æ­£ç³»æ•°1.337ï¼ˆæ ¹æ®å®é™…APIæµ‹è¯•å¾—å‡ºï¼‰
                        corrected_tokens = int(base_tokens * 1.337)
                        return corrected_tokens
                    except ImportError:
                        pass  # fallbackåˆ°ç²—ç•¥ä¼°ç®—
            # ç²—ç•¥ä¼°ç®—ä½œä¸ºfallback
            return len(text) // 4 + 1
        
        # ç²¾ç¡®è®¡ç®—æ¯ä¸ªéƒ¨åˆ†
        size = estimate_tokens_precise(self.system_prompt)  # ç³»ç»Ÿæç¤ºè¯
        size += estimate_tokens_precise(base_prompt)
        
        if event_content and event_content != 'Normal business conditions':
            size += estimate_tokens_precise(event_content)
        
        # è®¡ç®—å…¨å±€å¯¹è¯å†å²å¤§å°
        for interaction in self.global_conversation_history:
            size += estimate_tokens_precise(interaction['llm_response'])
            size += estimate_tokens_precise(interaction['manager_feedback'])
        
        # è®¡ç®—æ–‡ä»¶å†…å®¹å¤§å°
        if files:
            for file_obj in files:
                size += estimate_tokens_precise(file_obj.content)
        
        # åŠ ä¸Šæ ¼å¼åŒ–å­—ç¬¦çš„ä¼°ç®—
        size += 1000  # æ ¼å¼åŒ–å­—ç¬¦å’Œå…¶ä»–overhead
        
        return size
    
    def _estimate_messages_tokens(self, messages: List[Dict[str, str]]) -> int:
        """ä¼°ç®—ChatGPTé£æ ¼æ¶ˆæ¯åˆ—è¡¨çš„tokenæ•°"""
        total_tokens = 0
        for msg in messages:
            # æ¯æ¡æ¶ˆæ¯çš„å†…å®¹ + roleæ ‡è®°çš„å¼€é”€
            total_tokens += len(msg['content']) // 4 + 10  # 10 tokens for role and formatting
        return total_tokens
    
    def _truncate_global_history(self, max_history_tokens: int = 100000):
        """æŒ‰taskå•ä½æˆªæ–­å…¨å±€å¯¹è¯å†å²ï¼Œä½¿ç”¨tiktokenç²¾ç¡®è®¡ç®—"""
        if not self.global_conversation_history:
            return
        
        # ä½¿ç”¨tiktokenç²¾ç¡®è®¡ç®—tokenæ•°ï¼ˆåªå¯¹GPTæ¨¡å‹ï¼‰
        def estimate_tokens_precise(text: str) -> int:
            if hasattr(self, 'llm_client') and hasattr(self.llm_client, 'provider'):
                if self.llm_client.provider in ['openai', 'azure', 'openrouter'] and 'gpt' in str(getattr(self.llm_client, 'model', '')).lower():
                    try:
                        import tiktoken
                        encoding = tiktoken.encoding_for_model("gpt-4o")
                        return len(encoding.encode(text))
                    except ImportError:
                        if hasattr(self, '_logger') and self._logger:
                            self._logger.log_warning("tiktoken not installed, using rough estimation")
                        pass
            # ç²—ç•¥ä¼°ç®—ä½œä¸ºfallback
            return len(text) // 4 + 1
        
        # è®¡ç®—å½“å‰å†å²å¤§å°
        total_tokens = 0
        for interaction in self.global_conversation_history:
            total_tokens += estimate_tokens_precise(interaction['llm_response'])
            total_tokens += estimate_tokens_precise(interaction['manager_feedback'])
        
        if hasattr(self, '_logger') and self._logger:
            self._logger.log_info(f"[LLM_TRUNCATE] Current history: {total_tokens} tokens, limit: {max_history_tokens}")
        
        # å¦‚æœä¸è¶…è¿‡é™åˆ¶ï¼Œç›´æ¥è¿”å›
        if total_tokens <= max_history_tokens:
            return
        
        # æŒ‰taskå•ä½åˆ†ç»„å†å²è®°å½•
        tasks_dict = {}
        for interaction in self.global_conversation_history:
            task_id = interaction['task_id']
            if task_id not in tasks_dict:
                tasks_dict[task_id] = []
            tasks_dict[task_id].append(interaction)
        
        # æŒ‰taské¡ºåºåˆ é™¤ï¼ˆä»æœ€æ—©çš„taskå¼€å§‹ï¼‰
        task_ids = list(tasks_dict.keys())  # ä¿æŒåŸå§‹é¡ºåº
        removed_tasks = 0
        
        for task_id in task_ids:
            if total_tokens <= max_history_tokens:
                break
                
            # è®¡ç®—è¿™ä¸ªtaskçš„tokenæ•°
            task_tokens = 0
            for interaction in tasks_dict[task_id]:
                task_tokens += estimate_tokens_precise(interaction['llm_response'])
                task_tokens += estimate_tokens_precise(interaction['manager_feedback'])
            
            # ä»global_conversation_historyä¸­åˆ é™¤è¿™ä¸ªtaskçš„æ‰€æœ‰rounds
            original_length = len(self.global_conversation_history)
            self.global_conversation_history = [
                interaction for interaction in self.global_conversation_history 
                if interaction['task_id'] != task_id
            ]
            removed_count = original_length - len(self.global_conversation_history)
            
            total_tokens -= task_tokens
            removed_tasks += 1
            
            if hasattr(self, '_logger') and self._logger:
                self._logger.log_warning(f"[LLM_TRUNCATE] Removed task {task_id}: {removed_count} rounds, {task_tokens} tokens")
        
        if hasattr(self, '_logger') and self._logger:
            self._logger.log_info(f"[LLM_TRUNCATE] Final: {total_tokens} tokens, removed {removed_tasks} tasks")
    
    def add_interaction_to_global_history(self, task_id: str, round_num: int, llm_response: str, manager_feedback: str, task_complete: bool):
        """æ·»åŠ äº¤äº’åˆ°å…¨å±€å†å²"""
        interaction_record = {
            'task_id': task_id,
            'round': round_num,
            'llm_response': llm_response,
            'manager_feedback': manager_feedback,
            'task_complete': task_complete
        }
        self.global_conversation_history.append(interaction_record)
    
    def _build_enhanced_prompt_with_history(self, task: Task, event_content: str) -> str:
        """æ„å»ºåŒ…å«å…¨å±€å†å²çš„å®Œæ•´prompt"""
        # 1. æ™ºèƒ½æˆªæ–­å†å²ä»¥é¿å…contextæº¢å‡º
        estimated_size = self._estimate_context_size(task.base_prompt, event_content, task.files)
        
        # è®¡ç®—å¹¶æ‰“å°contextä½¿ç”¨ç™¾åˆ†æ¯”ï¼ˆåªåœ¨LLMç»„ä»¶ä¸­ï¼‰
        if not hasattr(self.llm_client, 'model_limits') or not hasattr(self.llm_client.model_limits, 'max_output_tokens'):
            raise ValueError("LLM client missing model_limits.max_output_tokens attribute")
        max_output_tokens = self.llm_client.model_limits.max_output_tokens
        available_context = self.context_limit - max_output_tokens
        usage_ratio = estimated_size / available_context
        print(f"[CONTEXT_MGMT] Current usage: {usage_ratio:.1%} ({estimated_size}/{available_context})")
        
        if hasattr(self, '_logger') and self._logger:
            self._logger.log_info(f"Estimated context size: ~{estimated_size} tokens")
        
        # åŠ¨æ€è®¡ç®—context limité˜ˆå€¼ï¼ˆä¿ç•™20%ä½™é‡ï¼‰
        context_threshold = int(self.context_limit * 0.8)
        if estimated_size > context_threshold:
            if hasattr(self, '_logger') and self._logger:
                self._logger.log_warning(f"Context approaching limit ({estimated_size}/{context_threshold}), truncating history...")
            self._truncate_global_history(max_history_tokens=30000)
            estimated_size_after = self._estimate_context_size(task.base_prompt, event_content, task.files)
            if hasattr(self, '_logger') and self._logger:
                self._logger.log_info(f"Context size after truncation: ~{estimated_size_after} tokens")
        
        # 2. æ„å»ºåŸºç¡€prompt
        if event_content and event_content != 'Normal business conditions':
            enhanced_prompt = f"""{task.base_prompt}

=== CURRENT SITUATION ===
{event_content}"""
        else:
            enhanced_prompt = task.base_prompt
        
        # 3. æ·»åŠ å…¨å±€å¯¹è¯å†å² (å¦‚æœæœ‰)
        if self.global_conversation_history:
            conversation_context = []
            for interaction in self.global_conversation_history:
                task_info = interaction.get('task_id')
                if not task_info:
                    raise ValueError(f"Interaction missing task_id at index {i}")
                round_info = interaction.get('round', 1)
                conversation_context.append(f"[{task_info} - Round {round_info}]")
                conversation_context.append(f"Assistant: {interaction['llm_response']}")
                conversation_context.append("")
                conversation_context.append(f"Manager: {interaction['manager_feedback']}")
                conversation_context.append("")
            
            enhanced_prompt += f"""

=== CONVERSATION HISTORY ===
{chr(10).join(conversation_context)}

[Current Task: {task.title}]
Please continue based on the conversation history above."""
        else:
            enhanced_prompt += f"""

[Current Task: {task.title}]
Please provide a comprehensive response to complete this task."""
        
        return enhanced_prompt

    def process_task_with_event(self, task: Task, event_content: str) -> Dict[str, Any]:
        """
        å¤„ç†å¸¦æœ‰äº‹ä»¶çš„ä»»åŠ¡ - è‡ªåŠ¨ç®¡ç†contextå’Œå†å²
        
        Args:
            task: Task object with prompt and files
            event_content: Event content (e.g., pressure situation)
            
        Returns:
            Dictionary containing LLM response and metadata
        """
        try:
            # æ„å»ºåŒ…å«å…¨å±€å†å²çš„enhanced prompt
            enhanced_prompt = self._build_enhanced_prompt_with_history(task, event_content)
            
            # ä½¿ç”¨enhanced promptæ–¹æ³•å¤„ç†ä»»åŠ¡
            return self.process_task_with_enhanced_prompt(task, enhanced_prompt, manager_feedback_history=None)
            
        except Exception as e:
            if hasattr(self, '_logger') and self._logger:
                self._logger.log_error(e, "Error in task processing with event")
            raise

    def process_task(self, 
                    task: Task, 
                    manager_feedback_history: Optional[List[str]]) -> Dict[str, Any]:
        """
        Process a task with file attachments using unified LLM client with retry mechanism
        
        Args:
            task: Task object with prompt and files
            manager_feedback_history: Previous feedback from manager
            
        Returns:
            Dictionary containing LLM response and metadata
        """
        
        try:
            # ğŸ†• ä½¿ç”¨é‡è¯•æœºåˆ¶è¿›è¡Œä»»åŠ¡å¤„ç†
            result = self.retry_handler.retry_with_warnings(
                self._single_task_attempt,
                "LLM",
                "task processing",
                task, manager_feedback_history
            )
            return result
                
        except Exception as e:
            error_msg = f"LLM processing failed after retries: {str(e)}"
            # ç°åœ¨è¿™æ˜¯çœŸæ­£çš„å¤±è´¥ï¼Œå¯ä»¥è¿”å›é”™è¯¯
            return {
                'response': f"I apologize, but I encountered an error processing this task: {error_msg}",
                'prompt_used': "",
                'files_processed': 0,
                'success': False,
                'error': error_msg,
                'llm_metadata': None
            }
    
    def _single_task_attempt(self, task: Task, manager_feedback_history: Optional[List[str]]) -> Dict[str, Any]:
        """å•æ¬¡ä»»åŠ¡å¤„ç†å°è¯• - è¢«é‡è¯•æœºåˆ¶è°ƒç”¨"""
        
        # 1. è·å–ä»»åŠ¡prompt - ä½¿ç”¨æ­£ç¡®çš„Taskç»“æ„
        base_prompt = task.base_prompt
        
        # 2. Build file context
        file_context = self._build_file_context(task.files)
        
        # 3. Build full prompt
        full_prompt = self._build_full_prompt(base_prompt, file_context)
        
        # 4. Add manager feedback context if available
        if manager_feedback_history:
            feedback_context = self._build_feedback_context(manager_feedback_history)
            full_prompt = feedback_context + "\n\n" + full_prompt
        
        # 5. Prepare messages for LLM client
        messages = [
            {"role": "user", "content": full_prompt}
        ]
        
        # 5.5. Check context size before LLM call
        estimated_tokens = self.llm_client.estimate_tokens(self.system_prompt + full_prompt)
        
        if estimated_tokens > self.context_limit:
            # Contextæº¢å‡º - è®°å½•è­¦å‘Šä½†ä¸æŠ›å‡ºé”™è¯¯ï¼ŒLLMåº”è¯¥èƒ½å¤„ç†é•¿å¯¹è¯
            if hasattr(self, '_logger') and self._logger:
                self._logger.log_warning(f"Context approaching limit: {estimated_tokens}/{self.context_limit} tokens, prompt length: {len(full_prompt)} chars")
        
        # å¦‚æœæœ‰loggerå®ä¾‹ï¼Œè®°å½•LLMè¾“å…¥
        if hasattr(self, '_logger') and self._logger:
            self._logger.log_llm_input(self.system_prompt, messages, self.model_name, self.max_tokens)

        # 6. Call unified LLM client with complete response requirement
        llm_result = self.llm_client.complete_chat(
            messages=messages,
            model=self.model_name,
            max_tokens=self.max_tokens,
            temperature=None,  # ä½¿ç”¨OpenAIé»˜è®¤å€¼
            system_role=self.system_prompt,
            require_complete_response=True,  # Enable multi-round concatenation for complete responses
            caller="LLM"
        )
        
        if not llm_result['success']:
            # æŠ›å‡ºå¼‚å¸¸è®©é‡è¯•æœºåˆ¶å¤„ç†
            raise RuntimeError(f"LLM call failed: {llm_result['error']}")
        
        llm_response = llm_result['content']
        
        # 7. Store conversation history - å®Œæ•´ä¿å­˜ï¼Œä¸æˆªæ–­
        self.conversation_history.append({
            'task_sequence_num': task.task_sequence_num,
            'task_id': task.title,
            'prompt_full': full_prompt,           # å®Œæ•´prompt
            'response_full': llm_response,      # å®Œæ•´å›å¤
            'files_provided': len(task.files) if task.files else 0,
            'llm_rounds': llm_result['total_rounds'],
            'tokens_used': llm_result['tokens_used']
        })
        
        return {
            'response': llm_response,
            'prompt_used': full_prompt,
            'files_processed': len(task.files) if task.files else 0,
            'success': True,
            'error': None,
            'llm_metadata': {
                'is_complete': llm_result['is_complete'],
                'is_truncated': llm_result['is_truncated'],
                'total_rounds': llm_result['total_rounds'],
                'tokens_used': llm_result['tokens_used'],
                'finish_reason': llm_result['finish_reason']
            }
        }
    
    def process_task_with_enhanced_prompt(self,
                                        task: Task,
                                        enhanced_prompt: str,
                                        manager_feedback_history: Optional[List[str]]) -> Dict[str, Any]:
        """
        Process a task with a pre-enhanced prompt (using Event System)
        
        Args:
            task: Task object with files and metadata
            enhanced_prompt: Already enhanced prompt (base_prompt + event if applicable)
            manager_feedback_history: Previous feedback from manager
            
        Returns:
            Dictionary containing LLM response and metadata
        """
        
        # ğŸ†• æ„å»ºChatGPTé£æ ¼çš„æ¶ˆæ¯åºåˆ—
        has_global_history = len(self.global_conversation_history) > 0
        has_task_feedback = manager_feedback_history and len(manager_feedback_history) > 0
        
        if has_global_history or has_task_feedback:
            # æœ‰å…¨å±€å†å²æˆ–Manageråé¦ˆå†å² - æ„å»ºäº¤é”™çš„å¯¹è¯åºåˆ—
            messages = self._build_chatgpt_style_messages(task, enhanced_prompt, manager_feedback_history)
            full_prompt = f"ChatGPT-style conversation ({len(messages)} messages)"
        else:
            # å®Œå…¨æ–°çš„å¼€å§‹ - ä½¿ç”¨ä¼ ç»Ÿçš„å•æ¶ˆæ¯æ ¼å¼
            file_context = self._build_file_context(task.files)
            full_prompt = self._build_full_prompt(enhanced_prompt, file_context)
            messages = [{"role": "user", "content": full_prompt}]
        
        # 4.5. Check context size before LLM call
        if has_global_history or has_task_feedback:
            # å¯¹äºChatGPTé£æ ¼æ¶ˆæ¯ï¼Œä¼°ç®—æ‰€æœ‰æ¶ˆæ¯çš„æ€»tokenæ•°
            total_content = self.system_prompt + "\n".join([msg["content"] for msg in messages])
            estimated_tokens = self.llm_client.estimate_tokens(total_content)
        else:
            estimated_tokens = self.llm_client.estimate_tokens(self.system_prompt + full_prompt)
        
        if estimated_tokens > self.context_limit:
            # Contextæº¢å‡º - è®°å½•è­¦å‘Šä½†ä¸æŠ›å‡ºé”™è¯¯ï¼ŒLLMåº”è¯¥èƒ½å¤„ç†é•¿å¯¹è¯
            if hasattr(self, '_logger') and self._logger:
                if has_global_history or has_task_feedback:
                    self._logger.log_warning(f"Context approaching limit: {estimated_tokens}/{self.context_limit} tokens, ChatGPT-style messages: {len(messages)} total")
                else:
                    self._logger.log_warning(f"Context approaching limit: {estimated_tokens}/{self.context_limit} tokens, prompt length: {len(full_prompt)} chars")
        
        # å¦‚æœæœ‰loggerå®ä¾‹ï¼Œè®°å½•LLMè¾“å…¥
        if hasattr(self, '_logger') and self._logger:
            self._logger.log_llm_input(self.system_prompt, messages, self.model_name, self.max_tokens)
        
        # 5. Call unified LLM client with smart retry for context overflow
        max_retries = 3
        estimated_tokens_for_scaling = estimated_tokens  # ä¿å­˜åˆå§‹ä¼°ç®—å€¼
        
        for attempt in range(max_retries):
            llm_result = self.llm_client.complete_chat(
                messages=messages,
                model=self.model_name,
                max_tokens=self.max_tokens,
                temperature=None,  # ä½¿ç”¨OpenAIé»˜è®¤å€¼
                system_role=self.system_prompt,
                require_complete_response=True,
                caller="LLM"
            )
            
            if llm_result['success']:
                break  # æˆåŠŸï¼Œé€€å‡ºé‡è¯•å¾ªç¯
            
            # æ£€æŸ¥æ˜¯å¦æ˜¯context overflowé”™è¯¯
            error_msg = llm_result.get('error') or 'Unknown error'
            if ('context_length_exceeded' in error_msg or 
                'maximum context length' in error_msg or 
                'messages resulted in' in error_msg):
                # è§£æcontext overflowä¿¡æ¯
                overflow_info = self.llm_client.parse_context_length_error(error_msg)
                
                if overflow_info:
                    if hasattr(self, '_logger') and self._logger:
                        self._logger.log_warning(
                            f"Context overflow on attempt {attempt + 1}: "
                            f"requested {overflow_info['requested_tokens']} tokens, "
                            f"max is {overflow_info['max_context_length']}, "
                            f"overflow by {overflow_info['overflow_tokens']} tokens"
                        )
                    
                    print(f"[LLM] Context overflow attempt {attempt + 1}: {overflow_info['requested_tokens']} > {overflow_info['max_context_length']}")
                    
                    # è®¡ç®—éœ€è¦åˆ é™¤çš„tokenæ•°
                    if attempt == 0:
                        # ç¬¬ä¸€æ¬¡ï¼šåŸºäºä¼°ç®—å€¼åˆ é™¤
                        tokens_to_remove = overflow_info['overflow_tokens'] + 5000  # åŠ 5000ä½™é‡
                    else:
                        # ç¬¬2-3æ¬¡ï¼šåŸºäºå®é™…/ä¼°ç®—æ¯”ä¾‹è°ƒæ•´
                        scale_factor = overflow_info['requested_tokens'] / estimated_tokens_for_scaling
                        tokens_to_remove = int(overflow_info['overflow_tokens'] * scale_factor * 1.2)  # åŠ 20%ä½™é‡
                        
                        if hasattr(self, '_logger') and self._logger:
                            self._logger.log_info(f"Scale factor: {scale_factor:.2f}, removing ~{tokens_to_remove} tokens")
                    
                    # æ ¹æ®æ¶ˆæ¯ç±»å‹è¿›è¡Œæˆªæ–­
                    if has_global_history or has_task_feedback:
                        # æ¿€è¿›æˆªæ–­ç­–ç•¥ï¼šç›´æ¥ç æ‰æ›´ä¹…è¿œçš„ä¸€åŠï¼Œä¿ç•™æœ€è¿‘çš„ä¸€åŠ
                        original_length = len(messages)
                        messages = messages[len(messages)//2:]
                        print(f"[LLM] Context overflow: cut {original_length//2} messages, kept {len(messages)}, retrying...")
                        
                        # é‡æ–°ä¼°ç®—tokenæ•°ï¼ˆä»…ç”¨äºä¸‹æ¬¡æº¢å‡ºåˆ¤æ–­ï¼‰
                        total_content = self.system_prompt + "\n".join([msg["content"] for msg in messages])
                        estimated_tokens_for_scaling = self.llm_client.estimate_tokens(total_content)
                    else:
                        # å•æ¶ˆæ¯æ¨¡å¼ï¼šæˆªæ–­å…¨å±€å†å²
                        self._truncate_global_history(max_history_tokens=30000)
                        print(f"[LLM] Truncating global history, retrying...")
                        
                        # é‡æ–°æ„å»ºprompt
                        enhanced_prompt = self._build_enhanced_prompt_with_history(task, enhanced_prompt.split('\n')[0])
                        file_context = self._build_file_context(task.files)
                        full_prompt = self._build_full_prompt(enhanced_prompt, file_context)
                        messages = [{"role": "user", "content": full_prompt}]
                        
                        estimated_tokens_for_scaling = self.llm_client.estimate_tokens(self.system_prompt + full_prompt)
                    
                    if attempt == max_retries - 1:
                        # æœ€åä¸€æ¬¡å°è¯•å¤±è´¥
                        raise RuntimeError(f"LLM call failed after {max_retries} retries due to context overflow")
                    continue  # ç»§ç»­ä¸‹ä¸€æ¬¡é‡è¯•
            
            # écontext overflowé”™è¯¯ï¼Œç›´æ¥æŠ›å‡º
            raise RuntimeError(f"LLM call failed: {llm_result['error']}")
        
        llm_response = llm_result['content']
        
        # 5.5. ç®¡ç†å½“å‰ä»»åŠ¡å›å¤å†å²
        if self.current_task_id != task.title:
            # æ–°ä»»åŠ¡å¼€å§‹ï¼Œé‡ç½®å½“å‰ä»»åŠ¡å›å¤å†å²
            self.current_task_id = task.title
            self.current_task_responses = []
        
        # æ·»åŠ å½“å‰å›å¤åˆ°ä»»åŠ¡å†…å†å²
        self.current_task_responses.append(llm_response)
        
        # 6. Store conversation history - å®Œæ•´ä¿å­˜ï¼Œä¸æˆªæ–­
        self.conversation_history.append({
            'task_sequence_num': task.task_sequence_num,
            'task_id': task.title,
            'prompt_full': full_prompt if not (has_global_history or has_task_feedback) else f"ChatGPT messages: {len(messages)}",
            'response_full': llm_response,      # å®Œæ•´å›å¤
            'files_provided': len(task.files) if task.files else 0,
            'llm_rounds': llm_result['total_rounds'],
            'tokens_used': llm_result['tokens_used']
        })
        
        return {
            'response': llm_response,
            'prompt_used': full_prompt if not (has_global_history or has_task_feedback) else f"ChatGPT-style conversation ({len(messages)} messages)",
            'messages_used': messages if (has_global_history or has_task_feedback) else None,
            'files_processed': len(task.files) if task.files else 0,
            'success': True,
            'error': None,
            'llm_metadata': {
                'is_complete': llm_result['is_complete'],
                'is_truncated': llm_result['is_truncated'],
                'total_rounds': llm_result['total_rounds'],
                'tokens_used': llm_result['tokens_used'],
                'finish_reason': llm_result['finish_reason']
            }
        }
            

    
    def _build_file_context(self, files: Optional[List[TaskFile]]) -> str:
        """Build file context section for the prompt"""
        if not files:
            return ""
        
        file_context = "\n\n=== ATTACHED FILES FOR ANALYSIS ===\n"
        
        for file_obj in files:
            file_context += f"\n--- FILE: {file_obj.filename} ---\n"
            # File description field removed - only use filename and content
            file_context += f"Type: {file_obj.file_type}\n"
            file_context += f"Content:\n{file_obj.content}\n"
        
        file_context += "\n=== END OF ATTACHED FILES ===\n"
        return file_context
    
    def _build_full_prompt(self, base_prompt: str, file_context: str) -> str:
        """Combine base prompt with file context"""
        if file_context:
            return f"{base_prompt}{file_context}\n\nPlease analyze the attached files and provide your complete response based on the task requirements above. Take your time to provide a thorough and comprehensive analysis."
        else:
            return base_prompt
    
    def _build_feedback_context(self, feedback_history: List[str]) -> str:
        """Build context from follow-up requests (legacy manager feedback removed)"""
        if not feedback_history:
            return ""
        
        context = "=== FOLLOW-UP REQUESTS ===\n"
        for i, followup in enumerate(feedback_history, 1):
            context += f"Request {i}: {followup}\n"
        context += "=== END REQUESTS ===\n"
        context += "Please address these follow-up requests in your current response."
        
        return context
    
    def _smart_truncate_messages(self, messages: List[Dict[str, str]], tokens_to_remove: int) -> List[Dict[str, str]]:
        """
        æ™ºèƒ½æˆªæ–­ChatGPTé£æ ¼æ¶ˆæ¯ï¼Œä»æœ€æ—©çš„æ¶ˆæ¯å¼€å§‹åˆ é™¤
        
        Args:
            messages: æ¶ˆæ¯åˆ—è¡¨
            tokens_to_remove: éœ€è¦åˆ é™¤çš„tokenæ•°
            
        Returns:
            æˆªæ–­åçš„æ¶ˆæ¯åˆ—è¡¨
        """
        if len(messages) <= 2:  # è‡³å°‘ä¿ç•™å½“å‰ä»»åŠ¡çš„æ¶ˆæ¯
            return messages
        
        truncated_messages = messages.copy()
        tokens_removed = 0
        
        # ä»æœ€æ—©çš„æ¶ˆæ¯å¼€å§‹åˆ é™¤ï¼ˆä¿æŒuser-assistantå¯¹çš„å®Œæ•´æ€§ï¼‰
        while tokens_removed < tokens_to_remove and len(truncated_messages) > 2:
            # åˆ é™¤æœ€æ—©çš„useræ¶ˆæ¯
            if truncated_messages[0]['role'] == 'user':
                removed_content = truncated_messages.pop(0)
                tokens_removed += len(removed_content['content']) // 4
                
                # å¦‚æœä¸‹ä¸€æ¡æ˜¯assistantæ¶ˆæ¯ï¼Œä¹Ÿåˆ é™¤å®ƒï¼ˆä¿æŒå¯¹è¯å®Œæ•´æ€§ï¼‰
                if len(truncated_messages) > 0 and truncated_messages[0]['role'] == 'assistant':
                    removed_content = truncated_messages.pop(0)
                    tokens_removed += len(removed_content['content']) // 4
            else:
                # å¦‚æœç¬¬ä¸€æ¡ä¸æ˜¯useræ¶ˆæ¯ï¼ˆä¸åº”è¯¥å‘ç”Ÿï¼‰ï¼Œç›´æ¥åˆ é™¤
                removed_content = truncated_messages.pop(0)
                tokens_removed += len(removed_content['content']) // 4
        
        if hasattr(self, '_logger') and self._logger:
            self._logger.log_warning(f"Truncated messages: removed ~{tokens_removed} tokens, {len(messages) - len(truncated_messages)} messages")
        
        return truncated_messages
    
    def _build_chatgpt_style_messages(self, task: Task, enhanced_prompt: str, manager_feedback_history: List[str]) -> List[Dict[str, str]]:
        """
        æ„å»ºChatGPTé£æ ¼çš„æ¶ˆæ¯åºåˆ— - å®Œæ•´çš„ä»»åŠ¡åºåˆ—å†å²
        
        å®ç°æ¶æ„ï¼š
        Task 1: R1 â†’ LLM â†’ Manager â†’ R2 â†’ LLM â†’ Manager â†’ R3 â†’ LLM â†’ Manager(å®Œæˆ)
        Task 2: R1 â†’ LLM â†’ Manager â†’ R2 â†’ LLM â†’ Manager(å®Œæˆ)  
        Task 3: R1 â†’ LLM â†’ Manager(å®Œæˆ)
        Current Task: R1 â†’ LLM â†’ ?
        
        Args:
            task: å½“å‰Task object
            enhanced_prompt: å½“å‰ä»»åŠ¡çš„enhanced prompt  
            manager_feedback_history: å½“å‰ä»»åŠ¡å†…çš„Manageråé¦ˆå†å²
            
        Returns:
            ChatGPTé£æ ¼çš„æ¶ˆæ¯åˆ—è¡¨
        """
        messages = []
        
        # 1. æ·»åŠ æ‰€æœ‰å·²å®Œæˆçš„ä»»åŠ¡å†å²ï¼ˆæŒ‰æ—¶é—´é¡ºåºï¼‰
        for interaction in self.global_conversation_history:
            task_id = interaction['task_id']  # å¿…é¡»å­˜åœ¨
            round_num = interaction.get('round', 1)
            
            # ä»»åŠ¡å¼€å§‹æ¶ˆæ¯ï¼ˆæ¯è½®éƒ½è¦é‡æ–°è¯´æ˜ä»»åŠ¡ï¼‰
            task_content = f"[{task_id} Round {round_num}] Please continue with your assigned task."
            messages.append({
                "role": "user",
                "content": task_content
            })
            
            # LLMçš„å›å¤
            llm_response = interaction.get('llm_response')
            if llm_response is None:
                raise ValueError(f"Interaction {task_id} missing llm_response")
            if llm_response:
                messages.append({
                    "role": "assistant", 
                    "content": llm_response
                })
            
            # Managerçš„åé¦ˆ
            manager_feedback = interaction.get('manager_feedback')
            if manager_feedback is None:
                raise ValueError(f"Interaction {task_id} missing manager_feedback")
            if manager_feedback:
                # å¦‚æœä»»åŠ¡å®Œæˆï¼Œæ˜ç¡®æ ‡æ³¨
                task_complete = interaction.get('task_complete', False)
                feedback_prefix = "Manager Feedback (Task Complete): " if task_complete else "Manager Feedback: "
                messages.append({
                    "role": "user",
                    "content": feedback_prefix + manager_feedback
                })
        
        # 2. æ·»åŠ å½“å‰ä»»åŠ¡çš„åˆå§‹æ¶ˆæ¯
        file_context = self._build_file_context(task.files)
        current_task_content = self._build_full_prompt(enhanced_prompt, file_context)
        
        # å½“å‰ä»»åŠ¡çš„è½®æ¬¡
        current_round = len(manager_feedback_history) + 1
        task_header = f"[{task.title} Round {current_round}]"
        
        messages.append({
            "role": "user",
            "content": f"{task_header} {current_task_content}"
        })
        
        # 3. æ·»åŠ å½“å‰ä»»åŠ¡å†…çš„å†å²è½®æ¬¡ï¼ˆå¦‚æœæœ‰ï¼‰
        for i, feedback in enumerate(manager_feedback_history):
            # æ·»åŠ å½“å‰ä»»åŠ¡ä¹‹å‰è½®æ¬¡çš„LLMå›å¤ï¼ˆå¦‚æœæœ‰ï¼‰
            if i < len(self.current_task_responses):
                messages.append({
                    "role": "assistant",
                    "content": self.current_task_responses[i]
                })
            
            # æ·»åŠ Managerçš„åé¦ˆ
            messages.append({
                "role": "user",
                "content": f"Manager Feedback: {feedback}"
            })
        
        return messages
    
    def get_conversation_summary(self) -> Dict[str, Any]:
        """Get summary of LLM's conversation history"""
        total_tokens = sum(h.get('tokens_used', 0) for h in self.conversation_history)
        total_rounds = sum(h.get('llm_rounds', 1) for h in self.conversation_history)
        
        return {
            'total_tasks_processed': len(self.conversation_history),
            'tasks_with_files': sum(1 for h in self.conversation_history if h['files_provided'] > 0),

            'llm_statistics': {
                'total_tokens_used': total_tokens,
                'total_llm_rounds': total_rounds,
                'average_rounds_per_task': total_rounds / max(len(self.conversation_history), 1)
            },
            'recent_tasks': self.conversation_history[-5:] if self.conversation_history else []
        }
    
    def get_llm_statistics(self) -> Dict[str, Any]:
        """Get LLM client statistics"""
        return self.llm_client.get_statistics()
    
    def reset_conversation(self):
        """Reset conversation history"""
        self.conversation_history = [] 