#!/usr/bin/env python3
"""
ğŸ”¥ ULTIMATE STRESS TEST ğŸ”¥
è¶…çº§ä¸¥è°¨å‹åŠ›æµ‹è¯• - è¦†ç›–æ‰€æœ‰å¯èƒ½çš„corner casesã€è¾¹ç•Œæ¡ä»¶ã€æé™æƒ…å†µ

æµ‹è¯•è¦†ç›–ï¼š
1. æ‰€æœ‰å‚æ•°ç»„åˆçš„æš´åŠ›æµ‹è¯• (30 categories x 5 pressure levels = 150 combinations)
2. è¾¹ç•Œå€¼å’Œæé™å€¼æµ‹è¯•
3. å¤§æ ·æœ¬é‡ç»Ÿè®¡å­¦éªŒè¯ (50,000+ samples per test)
4. å¤šçº¿ç¨‹å¹¶å‘å‹åŠ›æµ‹è¯•
5. å†…å­˜å’Œæ€§èƒ½å‹åŠ›æµ‹è¯•
6. å¼‚å¸¸å’Œé”™è¯¯è¾¹ç•Œæµ‹è¯•
7. ç§å­ä¸€è‡´æ€§å’Œéšæœºæ€§éªŒè¯
8. JSONç»“æ„å®Œæ•´æ€§éªŒè¯
9. æ•°å­¦åˆ†å¸ƒç²¾åº¦éªŒè¯ (åˆ°å°æ•°ç‚¹å6ä½)
10. ç³»ç»Ÿèµ„æºæé™æµ‹è¯•
"""

import sys
import os
import time
import json
import math
import threading
import multiprocessing
import resource
import gc
from concurrent.futures import ThreadPoolExecutor, ProcessPoolExecutor
from collections import defaultdict, Counter
from typing import Dict, List, Tuple, Set, Any, Optional
import datetime
import hashlib

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from tasks.event import EventSystem, UNCONTROL
from tasks.task import TaskLoader

# ğŸ”¥ ULTIMATE TEST CONFIGURATION ğŸ”¥
ULTIMATE_SAMPLE_SIZE = 50000  # 5ä¸‡æ ·æœ¬ç¡®ä¿æé«˜ç»Ÿè®¡ç²¾åº¦
STRESS_SAMPLE_SIZE = 100000   # 10ä¸‡æ ·æœ¬ç”¨äºå‹åŠ›æµ‹è¯•
TASK_STREAM_NAME = "market_research_series"
TOTAL_TASKS = 14
MAX_THREADS = min(32, multiprocessing.cpu_count() * 4)  # æœ€å¤§å¹¶å‘çº¿ç¨‹æ•°
MEMORY_LIMIT_GB = 4  # å†…å­˜ä½¿ç”¨é™åˆ¶

# å®Œæ•´çš„æµ‹è¯•å‚æ•°ç©ºé—´
ALL_CATEGORIES = ['GOAL_CONFLICT', 'MORAL_DILEMMA', 'COMPETITIVE_INTERACTION', 'AUTHORITY_DIRECTIVE', 'INFORMATION_GAP']
ALL_PRESSURE_LEVELS = ['LOW', 'MEDIUM', 'HIGH', 'CRITICAL']
ALL_P_EVENTS = [0.0, 0.001, 0.01, 0.1, 0.5, 0.9, 0.99, 0.999, 1.0]  # è¾¹ç•Œæ¦‚ç‡å€¼
EXTREME_SEEDS = [0, 1, 2, 42, 12345, 2**15, 2**16, 2**31-1, 2**32-1]  # æç«¯ç§å­å€¼

class UltimateStressTester:
    """ğŸ”¥ ç»ˆæå‹åŠ›æµ‹è¯•å™¨ - ä¸å…è®¸ä»»ä½•bugé€ƒè„±ï¼"""
    
    def __init__(self):
        self.task_loader = TaskLoader()
        self.task_stream = self.task_loader.load_task_stream(TASK_STREAM_NAME)
        self.test_results = {}
        self.performance_stats = {}
        self.error_log = []
        
        # è®¾ç½®å†…å­˜ç›‘æ§
        self.initial_memory = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
        
        print("ğŸ”¥ ULTIMATE STRESS TESTER INITIALIZED")
        print(f"   Max Threads: {MAX_THREADS}")
        print(f"   Memory Limit: {MEMORY_LIMIT_GB}GB")
        print(f"   CPU Cores: {multiprocessing.cpu_count()}")
        print(f"   Ultimate Sample Size: {ULTIMATE_SAMPLE_SIZE:,}")
        print(f"   Stress Sample Size: {STRESS_SAMPLE_SIZE:,}")
    
    def generate_all_possible_combinations(self) -> List[Dict]:
        """ç”Ÿæˆæ‰€æœ‰å¯èƒ½çš„å‚æ•°ç»„åˆ - ç©·å°½æ€§æµ‹è¯•"""
        combinations = []
        test_id = 0
        
        print("ğŸ§¬ Generating exhaustive parameter combinations...")
        
        # 1. æ­£å¸¸æ§åˆ¶ç»„åˆ - ç©·å°½æ‰€æœ‰å¯èƒ½
        control_categories = [UNCONTROL] + ALL_CATEGORIES
        control_pressures = [UNCONTROL] + ALL_PRESSURE_LEVELS
        
        for cat in control_categories:
            for pres in control_pressures:
                for p_event in ALL_P_EVENTS:
                    for seed in EXTREME_SEEDS:
                        test_id += 1
                        combinations.append({
                            'test_id': test_id,
                            'control_category': cat,
                            'control_pressure_level': pres,
                            'p_event': p_event,
                            'seed': seed,
                            'name': f"Test_{test_id:04d}_{cat}_{pres}_P{p_event}_S{seed}",
                            'test_type': 'exhaustive_normal'
                        })
        
        # 2. è¾¹ç•Œå’Œå¼‚å¸¸å€¼æµ‹è¯•
        boundary_tests = [
            # å­—ç¬¦ä¸²è¾¹ç•Œ
            {'control_category': '', 'control_pressure_level': UNCONTROL, 'expected_error': True, 'description': 'Empty category string'},
            {'control_category': UNCONTROL, 'control_pressure_level': '', 'expected_error': True, 'description': 'Empty pressure string'},
            {'control_category': ' ', 'control_pressure_level': UNCONTROL, 'expected_error': True, 'description': 'Whitespace category'},
            {'control_category': UNCONTROL, 'control_pressure_level': ' ', 'expected_error': True, 'description': 'Whitespace pressure'},
            
            # å¤§å°å†™æ•æ„Ÿæ€§æµ‹è¯•
            {'control_category': 'goal_conflict', 'control_pressure_level': UNCONTROL, 'expected_error': True, 'description': 'Lowercase category'},
            {'control_category': UNCONTROL, 'control_pressure_level': 'high', 'expected_error': True, 'description': 'Lowercase pressure'},
            {'control_category': 'Goal_Conflict', 'control_pressure_level': UNCONTROL, 'expected_error': True, 'description': 'Mixed case category'},
            
            # ç‰¹æ®Šå­—ç¬¦æµ‹è¯•
            {'control_category': 'GOAL-CONFLICT', 'control_pressure_level': UNCONTROL, 'expected_error': True, 'description': 'Hyphen in category'},
            {'control_category': 'GOAL_CONFLICT_', 'control_pressure_level': UNCONTROL, 'expected_error': True, 'description': 'Trailing underscore'},
            {'control_category': '_GOAL_CONFLICT', 'control_pressure_level': UNCONTROL, 'expected_error': True, 'description': 'Leading underscore'},
            
            # Unicodeå’Œç¼–ç æµ‹è¯•
            {'control_category': 'GOAL_CONFLICT\x00', 'control_pressure_level': UNCONTROL, 'expected_error': True, 'description': 'Null byte in category'},
            {'control_category': 'GOAL_CONFLICT\n', 'control_pressure_level': UNCONTROL, 'expected_error': True, 'description': 'Newline in category'},
            {'control_category': 'GOAL_CONFLICT\t', 'control_pressure_level': UNCONTROL, 'expected_error': True, 'description': 'Tab in category'},
            
            # æé•¿å­—ç¬¦ä¸²æµ‹è¯•
            {'control_category': 'A' * 1000, 'control_pressure_level': UNCONTROL, 'expected_error': True, 'description': 'Very long category string'},
            {'control_category': UNCONTROL, 'control_pressure_level': 'B' * 1000, 'expected_error': True, 'description': 'Very long pressure string'},
            
            # SQLæ³¨å…¥é£æ ¼æµ‹è¯• (è™½ç„¶ä¸æ˜¯æ•°æ®åº“ï¼Œä½†æµ‹è¯•å­—ç¬¦ä¸²å¤„ç†)
            {'control_category': "'; DROP TABLE events; --", 'control_pressure_level': UNCONTROL, 'expected_error': True, 'description': 'SQL injection style'},
            
            # æ•°å€¼å‹å­—ç¬¦ä¸²æµ‹è¯•
            {'control_category': '123', 'control_pressure_level': UNCONTROL, 'expected_error': True, 'description': 'Numeric string category'},
            {'control_category': UNCONTROL, 'control_pressure_level': '456', 'expected_error': True, 'description': 'Numeric string pressure'},
            
            # å¸ƒå°”å‹å­—ç¬¦ä¸²æµ‹è¯•
            {'control_category': 'True', 'control_pressure_level': UNCONTROL, 'expected_error': True, 'description': 'Boolean string category'},
            {'control_category': 'false', 'control_pressure_level': UNCONTROL, 'expected_error': True, 'description': 'Lowercase boolean'},
        ]
        
        for i, boundary_test in enumerate(boundary_tests):
            test_id += 1
            combinations.append({
                'test_id': test_id,
                'control_category': boundary_test['control_category'],
                'control_pressure_level': boundary_test['control_pressure_level'],
                'p_event': 1.0,
                'seed': 42,
                'name': f"Boundary_{test_id:04d}_{boundary_test['description'].replace(' ', '_')}",
                'test_type': 'boundary_error' if boundary_test.get('expected_error') else 'boundary_normal',
                'description': boundary_test['description']
            })
        
        print(f"ğŸ“Š Generated {len(combinations):,} total test combinations")
        print(f"   - Normal tests: {len([c for c in combinations if c['test_type'] == 'exhaustive_normal']):,}")
        print(f"   - Boundary tests: {len([c for c in combinations if c['test_type'].startswith('boundary')]):,}")
        
        return combinations
    
    def run_single_test_with_stats(self, config: Dict) -> Dict:
        """è¿è¡Œå•ä¸ªæµ‹è¯•å¹¶æ”¶é›†è¯¦ç»†ç»Ÿè®¡"""
        start_time = time.time()
        memory_before = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
        
        try:
            if config['test_type'] == 'boundary_error':
                # æœŸæœ›é”™è¯¯çš„æµ‹è¯•
                try:
                    event_system = EventSystem(
                        TASK_STREAM_NAME,
                        p_event=config['p_event'],
                        control_category=config['control_category'],
                        control_pressure_level=config['control_pressure_level'],
                        seed=config['seed'],
                        total_tasks=TOTAL_TASKS
                    )
                    
                    # å¦‚æœæ²¡æœ‰æŠ›å‡ºé”™è¯¯ï¼Œè¿™æ˜¯ä¸ªé—®é¢˜
                    return {
                        'test_id': config['test_id'],
                        'success': False,
                        'error': f"Expected error but test passed: {config['description']}",
                        'execution_time': time.time() - start_time,
                        'memory_usage': resource.getrusage(resource.RUSAGE_SELF).ru_maxrss - memory_before
                    }
                    
                except (ValueError, TypeError, AttributeError) as e:
                    # æ­£ç¡®æ•è·äº†é¢„æœŸé”™è¯¯
                    return {
                        'test_id': config['test_id'],
                        'success': True,
                        'result_type': 'expected_error',
                        'error_message': str(e),
                        'execution_time': time.time() - start_time,
                        'memory_usage': resource.getrusage(resource.RUSAGE_SELF).ru_maxrss - memory_before
                    }
            
            else:
                # æ­£å¸¸æµ‹è¯•æµç¨‹
                event_system = EventSystem(
                    TASK_STREAM_NAME,
                    p_event=config['p_event'],
                    control_category=config['control_category'],
                    control_pressure_level=config['control_pressure_level'],
                    seed=config['seed'],
                    total_tasks=TOTAL_TASKS
                )
                
                # å¤§æ ·æœ¬é‡ç»Ÿè®¡æµ‹è¯•
                category_counts = Counter()
                pressure_counts = Counter()
                event_triggered_count = 0
                event_distribution = Counter()
                phase_distribution = defaultdict(Counter)
                
                sample_size = ULTIMATE_SAMPLE_SIZE if config['test_type'] == 'exhaustive_normal' else 1000
                
                for sample_idx in range(sample_size):
                    # ä½¿ç”¨ç¡®å®šæ€§ç§å­åºåˆ—
                    test_seed = config['seed'] + sample_idx if config['seed'] is not None else None
                    
                    event_system_sample = EventSystem(
                        TASK_STREAM_NAME,
                        p_event=config['p_event'],
                        control_category=config['control_category'],
                        control_pressure_level=config['control_pressure_level'],
                        seed=test_seed,
                        total_tasks=TOTAL_TASKS
                    )
                    
                    for task_sequence_num in range(1, TOTAL_TASKS + 1):
                        phase = self.task_stream.get_phase_for_task(task_sequence_num)
                        event_obj, event_variant = event_system_sample.get_event_and_variant_for_phase(phase, task_sequence_num)
                        
                        if event_obj and event_variant:
                            event_triggered_count += 1
                            category_counts[event_variant['category']] += 1
                            pressure_counts[event_variant['pressure_level']] += 1
                            event_distribution[event_variant['name']] += 1
                            phase_distribution[phase][event_variant['category']] += 1
                
                total_samples = sample_size * TOTAL_TASKS
                actual_trigger_rate = event_triggered_count / total_samples if total_samples > 0 else 0
                
                # è®¡ç®—ç»Ÿè®¡æŒ‡æ ‡
                result = {
                    'test_id': config['test_id'],
                    'success': True,
                    'result_type': 'normal',
                    'config': config,
                    'total_samples': total_samples,
                    'event_triggered_count': event_triggered_count,
                    'actual_trigger_rate': actual_trigger_rate,
                    'expected_trigger_rate': config['p_event'],
                    'trigger_rate_deviation': abs(actual_trigger_rate - config['p_event']),
                    'category_distribution': dict(category_counts),
                    'pressure_distribution': dict(pressure_counts),
                    'event_distribution': dict(event_distribution),
                    'phase_distribution': {phase: dict(counts) for phase, counts in phase_distribution.items()},
                    'execution_time': time.time() - start_time,
                    'memory_usage': resource.getrusage(resource.RUSAGE_SELF).ru_maxrss - memory_before
                }
                
                # è¿›è¡Œä¸¥æ ¼çš„ç»Ÿè®¡å­¦éªŒè¯
                if total_samples > 0:
                    result.update(self.perform_statistical_analysis(result, config))
                
                return result
                
        except Exception as e:
            return {
                'test_id': config['test_id'],
                'success': False,
                'error': f"Unexpected error: {type(e).__name__}: {str(e)}",
                'execution_time': time.time() - start_time,
                'memory_usage': resource.getrusage(resource.RUSAGE_SELF).ru_maxrss - memory_before
            }
    
    def perform_statistical_analysis(self, result: Dict, config: Dict) -> Dict:
        """æ‰§è¡Œä¸¥æ ¼çš„ç»Ÿè®¡å­¦åˆ†æ"""
        stats = {}
        
        # 1. å¡æ–¹æ£€éªŒ
        if config['p_event'] > 0:
            # Categoryåˆ†å¸ƒæ£€éªŒ
            if config['control_category'] == UNCONTROL:
                expected_cat_prob = 1.0 / len(ALL_CATEGORIES)
                cat_chi_square = 0.0
                for category in ALL_CATEGORIES:
                    observed = result['category_distribution'].get(category, 0)
                    expected = expected_cat_prob * result['event_triggered_count']
                    if expected > 0:
                        cat_chi_square += (observed - expected) ** 2 / expected
                
                stats['category_chi_square'] = cat_chi_square
                stats['category_df'] = len(ALL_CATEGORIES) - 1
                stats['category_critical_999'] = 18.467  # df=4, 99.9%
                stats['category_passes_chi_square'] = cat_chi_square < 18.467
            
            # Pressureåˆ†å¸ƒæ£€éªŒ
            if config['control_pressure_level'] == UNCONTROL:
                expected_pres_prob = 1.0 / len(ALL_PRESSURE_LEVELS)
                pres_chi_square = 0.0
                for pressure in ALL_PRESSURE_LEVELS:
                    observed = result['pressure_distribution'].get(pressure, 0)
                    expected = expected_pres_prob * result['event_triggered_count']
                    if expected > 0:
                        pres_chi_square += (observed - expected) ** 2 / expected
                
                stats['pressure_chi_square'] = pres_chi_square
                stats['pressure_df'] = len(ALL_PRESSURE_LEVELS) - 1
                stats['pressure_critical_999'] = 16.266  # df=3, 99.9%
                stats['pressure_passes_chi_square'] = pres_chi_square < 16.266
        
        # 2. äºŒé¡¹åˆ†å¸ƒæ£€éªŒ (trigger rate)
        n = result['total_samples']
        p = config['p_event']
        observed_triggers = result['event_triggered_count']
        
        if n > 0 and 0 < p < 1:
            # è®¡ç®—95%ç½®ä¿¡åŒºé—´
            z_95 = 1.96
            std_error = math.sqrt(p * (1 - p) / n)
            lower_bound = p - z_95 * std_error
            upper_bound = p + z_95 * std_error
            
            actual_rate = observed_triggers / n
            stats['trigger_rate_in_95_ci'] = lower_bound <= actual_rate <= upper_bound
            stats['trigger_rate_95_ci'] = (lower_bound, upper_bound)
            stats['trigger_rate_z_score'] = (actual_rate - p) / std_error if std_error > 0 else 0
        
        # 3. å‡åŒ€æ€§æ£€éªŒ (Kolmogorov-Smirnov test approximation)
        if config['control_category'] == UNCONTROL and result['event_triggered_count'] > 0:
            cat_counts = [result['category_distribution'].get(cat, 0) for cat in ALL_CATEGORIES]
            total_cat_events = sum(cat_counts)
            if total_cat_events > 0:
                cat_probs = [count / total_cat_events for count in cat_counts]
                expected_prob = 1.0 / len(ALL_CATEGORIES)
                max_deviation = max(abs(prob - expected_prob) for prob in cat_probs)
                stats['category_max_deviation'] = max_deviation
                stats['category_uniformity_threshold'] = 0.05  # 5% threshold
                stats['category_passes_uniformity'] = max_deviation < 0.05
        
        return stats
    
    def run_parallel_stress_test(self, configs: List[Dict], max_workers: int = MAX_THREADS) -> Dict:
        """è¿è¡Œå¹¶è¡Œå‹åŠ›æµ‹è¯•"""
        print(f"ğŸ”¥ Starting parallel stress test with {max_workers} workers...")
        print(f"   Processing {len(configs):,} test configurations...")
        
        start_time = time.time()
        results = {}
        completed = 0
        
        def update_progress():
            if completed % 100 == 0:
                elapsed = time.time() - start_time
                rate = completed / elapsed if elapsed > 0 else 0
                eta = (len(configs) - completed) / rate if rate > 0 else 0
                print(f"   Progress: {completed:,}/{len(configs):,} ({completed/len(configs)*100:.1f}%) | "
                      f"Rate: {rate:.1f} tests/sec | ETA: {eta/60:.1f}min")
        
        with ThreadPoolExecutor(max_workers=max_workers) as executor:
            # æäº¤æ‰€æœ‰ä»»åŠ¡
            future_to_config = {executor.submit(self.run_single_test_with_stats, config): config for config in configs}
            
            # æ”¶é›†ç»“æœ
            for future in future_to_config:
                try:
                    result = future.result(timeout=300)  # 5åˆ†é’Ÿè¶…æ—¶
                    results[result['test_id']] = result
                    completed += 1
                    
                    if completed % 100 == 0:
                        update_progress()
                        
                except Exception as e:
                    config = future_to_config[future]
                    self.error_log.append(f"Test {config.get('test_id', 'unknown')} failed: {e}")
                    completed += 1
        
        total_time = time.time() - start_time
        print(f"âœ… Parallel stress test completed in {total_time:.2f} seconds")
        print(f"   Average rate: {len(configs)/total_time:.1f} tests/second")
        
        return results
    
    def run_memory_stress_test(self) -> Dict:
        """è¿è¡Œå†…å­˜å‹åŠ›æµ‹è¯•"""
        print("ğŸ’¾ Running memory stress test...")
        
        # åˆ›å»ºå¤§é‡EventSystemå®ä¾‹
        systems = []
        memory_stats = []
        
        try:
            for i in range(1000):  # åˆ›å»º1000ä¸ªå®ä¾‹
                system = EventSystem(
                    TASK_STREAM_NAME,
                    p_event=1.0,
                    control_category=UNCONTROL,
                    control_pressure_level=UNCONTROL,
                    seed=i,
                    total_tasks=TOTAL_TASKS
                )
                systems.append(system)
                
                if i % 100 == 0:
                    current_memory = resource.getrusage(resource.RUSAGE_SELF).ru_maxrss
                    memory_stats.append({
                        'instances': i + 1,
                        'memory_mb': current_memory / 1024 / 1024 if os.name != 'nt' else current_memory / 1024
                    })
            
            # æµ‹è¯•å¤§é‡äº‹ä»¶ç”Ÿæˆ
            total_events = 0
            for system in systems[:10]:  # åªæµ‹è¯•å‰10ä¸ªä»¥èŠ‚çœæ—¶é—´
                for task_num in range(1, TOTAL_TASKS + 1):
                    phase = self.task_stream.get_phase_for_task(task_num)
                    event_obj, event_variant = system.get_event_and_variant_for_phase(phase, task_num)
                    if event_obj:
                        total_events += 1
            
            return {
                'instances_created': len(systems),
                'total_events_generated': total_events,
                'memory_progression': memory_stats,
                'final_memory_mb': memory_stats[-1]['memory_mb'] if memory_stats else 0,
                'memory_per_instance_kb': (memory_stats[-1]['memory_mb'] * 1024 / len(systems)) if memory_stats and systems else 0
            }
            
        finally:
            # æ¸…ç†å†…å­˜
            del systems
            gc.collect()
    
    def run_seed_consistency_test(self) -> Dict:
        """è¿è¡Œç§å­ä¸€è‡´æ€§è¶…çº§ä¸¥æ ¼æµ‹è¯•"""
        print("ğŸ¯ Running ultra-strict seed consistency test...")
        
        consistency_results = {}
        test_seeds = [0, 1, 42, 12345, 2**31-1]
        
        for seed in test_seeds:
            print(f"   Testing seed: {seed}")
            
            # ç”Ÿæˆå¤šä¸ªå®Œå…¨ç›¸åŒçš„åºåˆ—
            sequences = []
            for run in range(10):
                event_system = EventSystem(
                    TASK_STREAM_NAME,
                    p_event=1.0,
                    control_category=UNCONTROL,
                    control_pressure_level=UNCONTROL,
                    seed=seed,
                    total_tasks=TOTAL_TASKS
                )
                
                sequence = []
                for task_num in range(1, TOTAL_TASKS + 1):
                    phase = self.task_stream.get_phase_for_task(task_num)
                    event_obj, event_variant = event_system.get_event_and_variant_for_phase(phase, task_num)
                    
                    if event_obj and event_variant:
                        sequence.append({
                            'task': task_num,
                            'phase': phase,
                            'category': event_variant['category'],
                            'pressure': event_variant['pressure_level'],
                            'event_name': event_variant['name'],
                            'content_hash': hashlib.md5(event_variant['content'].encode()).hexdigest()
                        })
                
                sequences.append(sequence)
            
            # éªŒè¯æ‰€æœ‰åºåˆ—å®Œå…¨ç›¸åŒ
            all_identical = all(seq == sequences[0] for seq in sequences)
            
            # è®¡ç®—åºåˆ—çš„å“ˆå¸Œå€¼ä»¥éªŒè¯
            sequence_hashes = [hashlib.md5(json.dumps(seq, sort_keys=True).encode()).hexdigest() for seq in sequences]
            hash_consistency = len(set(sequence_hashes)) == 1
            
            consistency_results[seed] = {
                'all_sequences_identical': all_identical,
                'hash_consistency': hash_consistency,
                'sequence_length': len(sequences[0]),
                'runs_tested': len(sequences),
                'sequence_hash': sequence_hashes[0] if hash_consistency else None
            }
        
        return consistency_results
    
    def generate_ultimate_report(self, all_results: Dict) -> str:
        """ç”Ÿæˆç»ˆææµ‹è¯•æŠ¥å‘Š"""
        report = []
        report.append("ğŸ”¥ ULTIMATE STRESS TEST REPORT ğŸ”¥")
        report.append("=" * 100)
        report.append(f"Test Date: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append(f"Python Version: {sys.version}")
        report.append(f"Platform: {os.name}")
        report.append("")
        
        # æ€»ä½“ç»Ÿè®¡
        total_tests = len(all_results.get('stress_results', {}))
        passed_tests = len([r for r in all_results.get('stress_results', {}).values() if r.get('success', False)])
        
        report.append("ğŸ“Š OVERALL STATISTICS:")
        report.append(f"  Total Tests Run: {total_tests:,}")
        report.append(f"  Passed Tests: {passed_tests:,}")
        report.append(f"  Failed Tests: {total_tests - passed_tests:,}")
        report.append(f"  Success Rate: {passed_tests/total_tests*100:.3f}%" if total_tests > 0 else "  Success Rate: N/A")
        report.append("")
        
        # æ€§èƒ½ç»Ÿè®¡
        if 'stress_results' in all_results:
            execution_times = [r.get('execution_time', 0) for r in all_results['stress_results'].values() if r.get('execution_time')]
            memory_usages = [r.get('memory_usage', 0) for r in all_results['stress_results'].values() if r.get('memory_usage')]
            
            if execution_times:
                report.append("âš¡ PERFORMANCE STATISTICS:")
                report.append(f"  Average Execution Time: {sum(execution_times)/len(execution_times):.4f}s")
                report.append(f"  Max Execution Time: {max(execution_times):.4f}s")
                report.append(f"  Min Execution Time: {min(execution_times):.4f}s")
                
            if memory_usages:
                report.append(f"  Average Memory Usage: {sum(memory_usages)/len(memory_usages):.2f}KB")
                report.append(f"  Max Memory Usage: {max(memory_usages):.2f}KB")
                report.append("")
        
        # ç»Ÿè®¡å­¦éªŒè¯ç»“æœ
        if 'stress_results' in all_results:
            chi_square_passes = 0
            trigger_rate_passes = 0
            uniformity_passes = 0
            
            for result in all_results['stress_results'].values():
                if result.get('category_passes_chi_square'):
                    chi_square_passes += 1
                if result.get('trigger_rate_in_95_ci'):
                    trigger_rate_passes += 1
                if result.get('category_passes_uniformity'):
                    uniformity_passes += 1
            
            report.append("ğŸ¯ STATISTICAL VALIDATION:")
            report.append(f"  Chi-Square Tests Passed: {chi_square_passes:,}")
            report.append(f"  Trigger Rate 95% CI Tests Passed: {trigger_rate_passes:,}")
            report.append(f"  Uniformity Tests Passed: {uniformity_passes:,}")
            report.append("")
        
        # ç§å­ä¸€è‡´æ€§ç»“æœ
        if 'seed_consistency' in all_results:
            report.append("ğŸ¯ SEED CONSISTENCY RESULTS:")
            for seed, result in all_results['seed_consistency'].items():
                status = "âœ… PERFECT" if result['all_sequences_identical'] and result['hash_consistency'] else "âŒ FAILED"
                report.append(f"  Seed {seed}: {status}")
            report.append("")
        
        # å†…å­˜å‹åŠ›æµ‹è¯•ç»“æœ
        if 'memory_stress' in all_results:
            mem_result = all_results['memory_stress']
            report.append("ğŸ’¾ MEMORY STRESS TEST:")
            report.append(f"  Instances Created: {mem_result.get('instances_created', 0):,}")
            report.append(f"  Events Generated: {mem_result.get('total_events_generated', 0):,}")
            report.append(f"  Final Memory Usage: {mem_result.get('final_memory_mb', 0):.2f}MB")
            report.append(f"  Memory per Instance: {mem_result.get('memory_per_instance_kb', 0):.2f}KB")
            report.append("")
        
        # é”™è¯¯æ—¥å¿—
        if self.error_log:
            report.append("âŒ ERROR LOG:")
            for error in self.error_log[:20]:  # åªæ˜¾ç¤ºå‰20ä¸ªé”™è¯¯
                report.append(f"  {error}")
            if len(self.error_log) > 20:
                report.append(f"  ... and {len(self.error_log) - 20} more errors")
            report.append("")
        
        # æœ€ç»ˆåˆ¤å†³
        if total_tests > 0:
            if passed_tests / total_tests >= 0.999:  # 99.9%é€šè¿‡ç‡
                report.append("ğŸ† FINAL VERDICT: ULTIMATE STRESS TEST PASSED! ğŸ†")
                report.append("   System demonstrates exceptional robustness and reliability.")
            elif passed_tests / total_tests >= 0.95:  # 95%é€šè¿‡ç‡
                report.append("âœ… FINAL VERDICT: STRESS TEST MOSTLY PASSED")
                report.append("   System shows good robustness with minor issues.")
            else:
                report.append("âŒ FINAL VERDICT: STRESS TEST FAILED")
                report.append("   System has significant robustness issues that need attention.")
        
        return "\n".join(report)
    
    def run_ultimate_stress_test(self) -> Dict:
        """è¿è¡Œç»ˆæå‹åŠ›æµ‹è¯•"""
        print("ğŸ”¥ğŸ”¥ğŸ”¥ STARTING ULTIMATE STRESS TEST ğŸ”¥ğŸ”¥ğŸ”¥")
        print("This is the most comprehensive test ever created for this system!")
        print()
        
        total_start_time = time.time()
        all_results = {}
        
        # 1. ç”Ÿæˆæ‰€æœ‰æµ‹è¯•ç»„åˆ
        self.test_combinations = self.generate_all_possible_combinations()
        
        # 2. è¿è¡Œå¹¶è¡Œå‹åŠ›æµ‹è¯•
        all_results['stress_results'] = self.run_parallel_stress_test(self.test_combinations)
        
        # 3. è¿è¡Œç§å­ä¸€è‡´æ€§æµ‹è¯•
        all_results['seed_consistency'] = self.run_seed_consistency_test()
        
        # 4. è¿è¡Œå†…å­˜å‹åŠ›æµ‹è¯•
        all_results['memory_stress'] = self.run_memory_stress_test()
        
        # 5. æ€§èƒ½ç»Ÿè®¡
        total_time = time.time() - total_start_time
        all_results['total_execution_time'] = total_time
        all_results['total_tests'] = len(self.test_combinations)
        all_results['average_time_per_test'] = total_time / len(self.test_combinations) if self.test_combinations else 0
        
        print(f"ğŸ‰ ULTIMATE STRESS TEST COMPLETED in {total_time:.2f} seconds!")
        
        return all_results

def main():
    """è¿è¡Œç»ˆæå‹åŠ›æµ‹è¯•"""
    print("ğŸ”¥ INITIALIZING ULTIMATE STRESS TESTER...")
    
    tester = UltimateStressTester()
    
    # è¿è¡Œç»ˆææµ‹è¯•
    results = tester.run_ultimate_stress_test()
    
    # ç”ŸæˆæŠ¥å‘Š
    report = tester.generate_ultimate_report(results)
    
    # è¾“å‡ºæŠ¥å‘Š
    print("\n" + "=" * 100)
    print(report)
    
    # ä¿å­˜ç»“æœ
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # ä¿å­˜JSONç»“æœ
    json_filename = f"TEST/ultimate_stress_results_{timestamp}.json"
    try:
        with open(json_filename, 'w', encoding='utf-8') as f:
            json.dump(results, f, indent=2, ensure_ascii=False, default=str)
        print(f"ğŸ’¾ Results saved to: {json_filename}")
    except Exception as e:
        print(f"âŒ Failed to save JSON results: {e}")
    
    # ä¿å­˜æŠ¥å‘Š
    report_filename = f"TEST/ultimate_stress_report_{timestamp}.txt"
    try:
        with open(report_filename, 'w', encoding='utf-8') as f:
            f.write(report)
        print(f"ğŸ“„ Report saved to: {report_filename}")
    except Exception as e:
        print(f"âŒ Failed to save report: {e}")

if __name__ == "__main__":
    main()