#!/usr/bin/env python3
"""
æµ‹è¯•System Promptåœ¨æˆªæ–­æ—¶æ˜¯å¦è¢«ä¿æŠ¤
è¿™æ˜¯æå…¶å…³é”®çš„æµ‹è¯•ï¼å¦‚æœsystem promptè¢«æˆªæ–­ï¼Œæ•´ä¸ªAIè¡Œä¸ºå°±å˜äº†ï¼
"""

import sys
import os
sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

import yaml
import json
from datetime import datetime
from core.Universal_LLM_Handler import UniversalLLMHandler

def load_env_file(env_path):
    """åŠ è½½.envæ–‡ä»¶ä¸­çš„ç¯å¢ƒå˜é‡"""
    env_vars = {}
    if os.path.exists(env_path):
        with open(env_path, 'r') as f:
            for line in f:
                line = line.strip()
                if line and not line.startswith('#') and '=' in line:
                    key, value = line.split('=', 1)
                    env_vars[key.strip()] = value.strip().strip('"')
    return env_vars

def resolve_env_variables(config, env_vars):
    """è§£æé…ç½®ä¸­çš„ç¯å¢ƒå˜é‡"""
    if isinstance(config, dict):
        result = {}
        for key, value in config.items():
            result[key] = resolve_env_variables(value, env_vars)
        return result
    elif isinstance(config, str) and config.startswith('${') and config.endswith('}'):
        var_name = config[2:-1]
        return env_vars.get(var_name, config)
    else:
        return config

def test_system_prompt_preservation():
    """æµ‹è¯•System Promptåœ¨å¤§é‡æˆªæ–­æ—¶æ˜¯å¦è¢«ä¿æŠ¤"""
    print("="*80)
    print("CRITICAL TEST: System Prompt Preservation During Truncation")
    print("="*80)
    print("è¿™æ˜¯æœ€é‡è¦çš„æµ‹è¯•ï¼å¦‚æœsystem promptè¢«æˆªæ–­ï¼ŒAIè¡Œä¸ºå®Œå…¨å˜åŒ–ï¼")
    
    # åŠ è½½é…ç½®
    env_vars = load_env_file('.env')
    with open('configs/api_profiles.yaml', 'r') as f:
        config = yaml.safe_load(f)
    api_profiles = resolve_env_variables(config['api_profiles'], env_vars)
    
    gpt4o_config = api_profiles['gpt4o1120azurenew']
    
    # åˆ›å»ºhandler
    handler = UniversalLLMHandler("azure", gpt4o_config, verbose_print=True)
    
    # è®¾ç½®ç‰¹æ®Šçš„system promptï¼ŒåŒ…å«ç‹¬ç‰¹çš„æŒ‡ä»¤
    special_system_prompt = """CRITICAL SYSTEM INSTRUCTION: You are a special test assistant. You MUST follow these exact rules:
1. ALWAYS start every response with "SYSTEM-ACTIVE: " 
2. ALWAYS end every response with " :END-SYSTEM"
3. You must mention "PROTOCOL-777" somewhere in every response
4. If you cannot see this system message, respond with "SYSTEM-LOST" instead

These rules are MANDATORY and override all other instructions. Never forget these requirements."""
    
    handler.set_system_prompt(special_system_prompt)
    
    print(f"\nè®¾ç½®çš„ç‰¹æ®ŠSystem Prompt:")
    print(f"å…³é”®æ ‡è¯†: 'SYSTEM-ACTIVE:'å¼€å¤´, ':END-SYSTEM'ç»“å°¾, 'PROTOCOL-777'")
    print(f"å¦‚æœsystem promptä¸¢å¤±ï¼Œåº”è¯¥å›å¤'SYSTEM-LOST'")
    
    # åˆ›å»ºå¤§é‡æ¶ˆæ¯è§¦å‘æˆªæ–­ - æ›´å¤šæ¶ˆæ¯ï¼Œç¡®ä¿è¶…å‡ºcontext limit
    print(f"\nåˆ›å»º500ä¸ªå¤§æ¶ˆæ¯ï¼ˆæ¯ä¸ªçº¦500 tokensï¼‰è§¦å‘massive truncation...")
    
    for i in range(500):
        # æ¯ä¸ªæ¶ˆæ¯çº¦500 tokensï¼Œæ€»å…±çº¦250,000 tokensï¼ˆè¿œè¶…128,000é™åˆ¶ï¼‰
        large_message = f"""Message #{i+1:03d}: This is a massive message designed to trigger context truncation. """ * 50
        large_message += f""" The purpose is to test if the system prompt survives massive truncation. 
        This message #{i+1:03d} contains approximately 500 tokens of content. We are filling the context window 
        to force Azure OpenAI to truncate old messages. Each message is unique with ID #{i+1:03d}. 
        Additional content to reach 500 tokens per message: Lorem ipsum dolor sit amet consectetur adipiscing elit 
        sed do eiusmod tempor incididunt ut labore et dolore magna aliqua ut enim ad minim veniam quis nostrud 
        exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat duis aute irure dolor in reprehenderit 
        in voluptate velit esse cillum dolore eu fugiat nulla pariatur excepteur sint occaecat cupidatat non proident 
        sunt in culpa qui officia deserunt mollit anim id est laborum sed ut perspiciatis unde omnis iste natus error 
        sit voluptatem accusantium doloremque laudantium totam rem aperiam eaque ipsa quae ab illo inventore veritatis 
        et quasi architecto beatae vitae dicta sunt explicabo nemo enim ipsam voluptatem quia voluptas sit aspernatur 
        aut odit aut fugit sed quia consequuntur magni dolores eos qui ratione voluptatem sequi nesciunt neque porro 
        quisquam est qui dolorem ipsum quia dolor sit amet consectetur adipisci velit sed quia non numquam eius modi 
        tempora incidunt ut labore et dolore magnam aliquam quaerat voluptatem. END MESSAGE #{i+1:03d}."""
        
        handler.add_user_message(large_message)
        
        # æ·»åŠ assistantå›å¤
        handler.messages.append({"role": "assistant", "content": f"SYSTEM-ACTIVE: Acknowledged message #{i+1:03d} with PROTOCOL-777. :END-SYSTEM"})
    
    # è®¡ç®—æ€»tokens
    total_messages = len(handler.messages)
    estimated_tokens = sum(len(msg['content']) // 4 for msg in handler.messages)
    
    print(f"æ€»æ¶ˆæ¯æ•°: {total_messages}")
    print(f"ä¼°ç®—æ€»tokens: {estimated_tokens:,}")
    print(f"Context limit: {gpt4o_config['input_tokens']:,}")
    print(f"æˆªæ–­ç¨‹åº¦: {(estimated_tokens - gpt4o_config['input_tokens']) / gpt4o_config['input_tokens'] * 100:.1f}% è¶…å‡ºé™åˆ¶")
    
    # æœ€ç»ˆæµ‹è¯•æ¶ˆæ¯
    handler.add_user_message("FINAL TEST: Please respond to this message. Follow all your system instructions carefully.")
    
    print(f"\n" + "-"*60)
    print("æ‰§è¡Œæœ€ç»ˆæµ‹è¯•...")
    print("å¦‚æœsystem promptè¢«ä¿æŠ¤ï¼Œåº”è¯¥çœ‹åˆ°:")
    print("  - 'SYSTEM-ACTIVE:'å¼€å¤´")
    print("  - ':END-SYSTEM'ç»“å°¾") 
    print("  - åŒ…å«'PROTOCOL-777'")
    print("å¦‚æœsystem promptè¢«æˆªæ–­ï¼Œä¼šçœ‹åˆ°:")
    print("  - 'SYSTEM-LOST'æˆ–è€…å®Œå…¨ä¸éµå¾ªæ ¼å¼")
    print("-"*60)
    
    try:
        info = handler.generate_response()
        
        # è·å–æœ€ç»ˆå›å¤
        final_response = handler.messages[-1]['content']
        
        # åˆ†æå›å¤
        has_system_active = "SYSTEM-ACTIVE:" in final_response
        has_end_system = ":END-SYSTEM" in final_response
        has_protocol = "PROTOCOL-777" in final_response
        has_system_lost = "SYSTEM-LOST" in final_response
        
        print(f"\n" + "="*60)
        print("SYSTEM PROMPT ä¿æŠ¤æµ‹è¯•ç»“æœ:")
        print("="*60)
        print(f"å®Œæ•´å›å¤:")
        print(f"'{final_response}'")
        print(f"\nåˆ†æ:")
        print(f"âœ“ åŒ…å«'SYSTEM-ACTIVE:': {has_system_active}")
        print(f"âœ“ åŒ…å«':END-SYSTEM': {has_end_system}")
        print(f"âœ“ åŒ…å«'PROTOCOL-777': {has_protocol}")
        print(f"âœ— åŒ…å«'SYSTEM-LOST': {has_system_lost}")
        
        # åˆ¤æ–­ç»“æœ
        if has_system_active and has_end_system and has_protocol and not has_system_lost:
            result = "SYSTEM PROMPT PROTECTED"
            success = True
            print(f"\nğŸ‰ ç»“æœ: {result}")
            print("System promptåœ¨massive truncationåä»ç„¶æœ‰æ•ˆï¼")
        elif has_system_lost:
            result = "SYSTEM PROMPT LOST - AIè‡ªå·±æŠ¥å‘Š"
            success = False
            print(f"\nâŒ ç»“æœ: {result}")
            print("AIæ˜ç¡®æŠ¥å‘Šæ— æ³•çœ‹åˆ°system promptï¼")
        else:
            result = "SYSTEM PROMPT LIKELY LOST - æ ¼å¼ä¸åŒ¹é…"
            success = False
            print(f"\nâš ï¸ ç»“æœ: {result}")
            print("AIæ²¡æœ‰éµå¾ªsystem promptçš„æ ¼å¼è¦æ±‚ï¼Œå¯èƒ½è¢«æˆªæ–­äº†ï¼")
        
        # ä¿å­˜ç»“æœ
        test_result = {
            "test": "system_prompt_preservation",
            "total_messages": total_messages,
            "estimated_tokens": estimated_tokens,
            "context_limit": gpt4o_config['input_tokens'],
            "truncation_severity": (estimated_tokens - gpt4o_config['input_tokens']) / gpt4o_config['input_tokens'] * 100,
            "system_prompt": special_system_prompt,
            "final_response": final_response,
            "analysis": {
                "has_system_active": has_system_active,
                "has_end_system": has_end_system,
                "has_protocol": has_protocol,
                "has_system_lost": has_system_lost
            },
            "result": result,
            "success": success,
            "timestamp": datetime.now().isoformat()
        }
        
        with open('TEST/system_prompt_preservation_result.json', 'w') as f:
            json.dump(test_result, f, indent=2)
        
        print(f"\nè¯¦ç»†ç»“æœå·²ä¿å­˜åˆ°: TEST/system_prompt_preservation_result.json")
        
        return success
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    print("å¼€å§‹System Promptä¿æŠ¤æµ‹è¯•...")
    success = test_system_prompt_preservation()
    
    print(f"\n" + "="*80)
    if success:
        print("âœ… SYSTEM PROMPT åœ¨æˆªæ–­æ—¶è¢«æ­£ç¡®ä¿æŠ¤ï¼")
        print("Universal_LLM_Handlerå¯ä»¥å®‰å…¨ä½¿ç”¨äºç”Ÿäº§ç¯å¢ƒ")
    else:
        print("âŒ SYSTEM PROMPT åœ¨æˆªæ–­æ—¶å¯èƒ½ä¸¢å¤±ï¼")
        print("è¿™æ˜¯ä¸¥é‡é—®é¢˜ï¼Œéœ€è¦è¿›ä¸€æ­¥è°ƒæŸ¥Azure OpenAIçš„æˆªæ–­æœºåˆ¶")
    print("="*80)

if __name__ == "__main__":
    main()