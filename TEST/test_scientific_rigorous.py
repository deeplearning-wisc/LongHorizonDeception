#!/usr/bin/env python3
"""
ç§‘ç ”çº§ä¸¥è°¨æµ‹è¯• + å·¥ç¨‹ç¦»å¥‡bugæ£€æµ‹
æ—¢ä¿è¯ç§‘å­¦å®éªŒçš„ç»Ÿè®¡å­¦ä¸¥è°¨æ€§ï¼Œåˆæ•è·å¯èƒ½çš„å·¥ç¨‹å®ç°bug
"""

import sys
import os
import json
import math
import time
import hashlib
from collections import defaultdict, Counter
from typing import Dict, List, Tuple, Any
import datetime

sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath(__file__))))

from tasks.event import EventSystem, UNCONTROL
from tasks.task import TaskLoader

# ç§‘ç ”çº§é…ç½® - è¶³å¤Ÿä¸¥è°¨ä½†ä¸è¿‡åˆ†
SCIENTIFIC_SAMPLE_SIZE = 10000  # 1ä¸‡æ ·æœ¬ç¡®ä¿ç»Ÿè®¡æ˜¾è‘—æ€§
TASK_STREAM_NAME = "market_research_series"
TOTAL_TASKS = 14
SIGNIFICANCE_LEVEL = 0.01  # 99%ç½®ä¿¡æ°´å¹³

# ç§‘å­¦å®éªŒå‚æ•°
ALL_CATEGORIES = ['GOAL_CONFLICT', 'MORAL_DILEMMA', 'COMPETITIVE_INTERACTION', 'AUTHORITY_DIRECTIVE', 'INFORMATION_GAP']
ALL_PRESSURE_LEVELS = ['LOW', 'MEDIUM', 'HIGH', 'CRITICAL']

class ScientificRigorousTester:
    """ç§‘ç ”çº§ä¸¥è°¨æµ‹è¯•å™¨ - å¹³è¡¡ç§‘å­¦æ€§å’Œå·¥ç¨‹å®ç”¨æ€§"""
    
    def __init__(self):
        self.task_loader = TaskLoader()
        self.task_stream = self.task_loader.load_task_stream(TASK_STREAM_NAME)
        self.results = {}
        
    def generate_scientific_test_matrix(self) -> List[Dict]:
        """ç”Ÿæˆç§‘å­¦å®éªŒè®¾è®¡çŸ©é˜µ"""
        test_matrix = []
        
        # 1. æ ¸å¿ƒç§‘å­¦å®éªŒï¼šæ‰€æœ‰æ§åˆ¶ç»„åˆ
        control_combinations = [
            # å®Œå…¨ä¸æ§åˆ¶
            {"control_category": UNCONTROL, "control_pressure_level": UNCONTROL},
            
            # å•ç»´åº¦æ§åˆ¶ - æ¯ä¸ªcategory
            *[{"control_category": cat, "control_pressure_level": UNCONTROL} for cat in ALL_CATEGORIES],
            
            # å•ç»´åº¦æ§åˆ¶ - æ¯ä¸ªpressure level  
            *[{"control_category": UNCONTROL, "control_pressure_level": pres} for pres in ALL_PRESSURE_LEVELS],
            
            # åŒç»´åº¦æ§åˆ¶ - å…³é”®ç»„åˆï¼ˆä¸éœ€è¦å…¨éƒ¨ï¼Œé€‰æ‹©ä»£è¡¨æ€§çš„ï¼‰
            {"control_category": "GOAL_CONFLICT", "control_pressure_level": "HIGH"},
            {"control_category": "MORAL_DILEMMA", "control_pressure_level": "LOW"},
            {"control_category": "INFORMATION_GAP", "control_pressure_level": "CRITICAL"},
        ]
        
        for i, combo in enumerate(control_combinations):
            test_matrix.append({
                'test_id': i + 1,
                'name': f"Scientific_Test_{i+1:02d}",
                'test_type': 'scientific_core',
                **combo,
                'p_event': 1.0,  # 100%è§¦å‘ç¡®ä¿ç»Ÿè®¡æœ‰æ•ˆæ€§
                'seed': 42  # å›ºå®šç§å­ç¡®ä¿å¯é‡ç°
            })
        
        # 2. å·¥ç¨‹è¾¹ç•Œæ¡ä»¶æµ‹è¯•
        boundary_tests = [
            # æ¦‚ç‡è¾¹ç•Œ
            {"control_category": UNCONTROL, "control_pressure_level": UNCONTROL, "p_event": 0.0, "seed": 1, "description": "Zero probability boundary"},
            {"control_category": UNCONTROL, "control_pressure_level": UNCONTROL, "p_event": 1.0, "seed": 1, "description": "Full probability boundary"},
            {"control_category": UNCONTROL, "control_pressure_level": UNCONTROL, "p_event": 0.5, "seed": 1, "description": "Half probability"},
            
            # ç§å­è¾¹ç•Œ
            {"control_category": UNCONTROL, "control_pressure_level": UNCONTROL, "p_event": 1.0, "seed": 0, "description": "Zero seed"},
            {"control_category": UNCONTROL, "control_pressure_level": UNCONTROL, "p_event": 1.0, "seed": 1, "description": "Minimum seed"},
            {"control_category": UNCONTROL, "control_pressure_level": UNCONTROL, "p_event": 1.0, "seed": 2**31-1, "description": "Maximum safe seed"},
            
            # ä»»åŠ¡æ•°é‡è¾¹ç•Œï¼ˆè™½ç„¶å›ºå®šä¸º14ï¼Œä½†æµ‹è¯•ä¸åŒphaseåˆ†å¸ƒï¼‰
            {"control_category": UNCONTROL, "control_pressure_level": UNCONTROL, "p_event": 1.0, "seed": 99, "description": "Phase boundary test"},
        ]
        
        for i, boundary in enumerate(boundary_tests):
            test_matrix.append({
                'test_id': len(control_combinations) + i + 1,
                'name': f"Boundary_Test_{i+1:02d}",
                'test_type': 'engineering_boundary',
                **boundary
            })
        
        # 3. ç¦»å¥‡bugæ£€æµ‹
        weird_bug_tests = [
            # å‚æ•°éªŒè¯æ˜¯å¦çœŸçš„æœ‰æ•ˆ
            {"control_category": "", "control_pressure_level": UNCONTROL, "p_event": 1.0, "seed": 42, "expected_error": True, "description": "Empty string category"},
            {"control_category": UNCONTROL, "control_pressure_level": "", "p_event": 1.0, "seed": 42, "expected_error": True, "description": "Empty string pressure"},
            {"control_category": "invalid_category", "control_pressure_level": UNCONTROL, "p_event": 1.0, "seed": 42, "expected_error": True, "description": "Invalid category"},
            {"control_category": UNCONTROL, "control_pressure_level": "INVALID_PRESSURE", "p_event": 1.0, "seed": 42, "expected_error": True, "description": "Invalid pressure"},
            
            # å¤§å°å†™æ•æ„Ÿbug
            {"control_category": "goal_conflict", "control_pressure_level": UNCONTROL, "p_event": 1.0, "seed": 42, "expected_error": True, "description": "Lowercase category"},
            {"control_category": UNCONTROL, "control_pressure_level": "high", "p_event": 1.0, "seed": 42, "expected_error": True, "description": "Lowercase pressure"},
            
            # Noneå€¼å¤„ç†bugï¼ˆç°åœ¨åº”è¯¥åœ¨æ„é€ å‡½æ•°å°±è¢«æ‹’ç»ï¼‰
            {"control_category": None, "control_pressure_level": UNCONTROL, "p_event": 1.0, "seed": 42, "expected_error": True, "description": "None category"},
            {"control_category": UNCONTROL, "control_pressure_level": None, "p_event": 1.0, "seed": 42, "expected_error": True, "description": "None pressure"},
        ]
        
        for i, weird in enumerate(weird_bug_tests):
            test_matrix.append({
                'test_id': len(control_combinations) + len(boundary_tests) + i + 1,
                'name': f"WeirdBug_Test_{i+1:02d}",
                'test_type': 'weird_bug_detection',
                **weird
            })
        
        return test_matrix
    
    def run_statistical_distribution_test(self, config: Dict) -> Dict:
        """è¿è¡Œç»Ÿè®¡å­¦åˆ†å¸ƒæµ‹è¯•"""
        print(f"ğŸ“Š Testing: {config['name']} - {config.get('description', '')}")
        
        if config.get('expected_error', False):
            # é”™è¯¯æµ‹è¯•
            try:
                event_system = EventSystem(
                    TASK_STREAM_NAME,
                    p_event=config['p_event'],
                    control_category=config['control_category'],
                    control_pressure_level=config['control_pressure_level'],
                    seed=config['seed'],
                    total_tasks=TOTAL_TASKS
                )
                return {
                    'test_id': config['test_id'],
                    'success': False,
                    'error': f"Expected error but test passed for: {config['description']}"
                }
            except Exception as e:
                return {
                    'test_id': config['test_id'],
                    'success': True,
                    'result_type': 'expected_error',
                    'error_caught': str(e)
                }
        
        # æ­£å¸¸æµ‹è¯•
        try:
            event_system = EventSystem(
                TASK_STREAM_NAME,
                p_event=config['p_event'],
                control_category=config['control_category'],
                control_pressure_level=config['control_pressure_level'],
                seed=config['seed'],
                total_tasks=TOTAL_TASKS
            )
            
            # æ”¶é›†ç»Ÿè®¡æ•°æ®
            category_counts = Counter()
            pressure_counts = Counter()
            event_triggered_count = 0
            phase_distribution = defaultdict(Counter)
            
            sample_size = SCIENTIFIC_SAMPLE_SIZE
            
            for sample_idx in range(sample_size):
                # æ¯æ¬¡ç”¨ä¸åŒä½†ç¡®å®šçš„ç§å­
                test_seed = config['seed'] + sample_idx
                
                test_event_system = EventSystem(
                    TASK_STREAM_NAME,
                    p_event=config['p_event'],
                    control_category=config['control_category'],
                    control_pressure_level=config['control_pressure_level'],
                    seed=test_seed,
                    total_tasks=TOTAL_TASKS
                )
                
                for task_sequence_num in range(1, TOTAL_TASKS + 1):
                    phase = self.task_stream.get_phase_for_task(task_sequence_num)
                    event_obj, event_variant = test_event_system.get_event_and_variant_for_phase(phase, task_sequence_num)
                    
                    if event_obj and event_variant:
                        event_triggered_count += 1
                        category_counts[event_variant['category']] += 1
                        pressure_counts[event_variant['pressure_level']] += 1
                        phase_distribution[phase][event_variant['category']] += 1
            
            total_samples = sample_size * TOTAL_TASKS
            
            # ç»Ÿè®¡å­¦åˆ†æ
            result = {
                'test_id': config['test_id'],
                'success': True,
                'config': config,
                'total_samples': total_samples,
                'event_triggered_count': event_triggered_count,
                'actual_trigger_rate': event_triggered_count / total_samples if total_samples > 0 else 0,
                'expected_trigger_rate': config['p_event'],
                'category_distribution': dict(category_counts),
                'pressure_distribution': dict(pressure_counts),
                'phase_distribution': {phase: dict(counts) for phase, counts in phase_distribution.items()},
            }
            
            # è¿›è¡Œç§‘å­¦ç»Ÿè®¡éªŒè¯
            result.update(self.perform_scientific_analysis(result, config))
            
            return result
            
        except Exception as e:
            return {
                'test_id': config['test_id'],
                'success': False,
                'error': f"Unexpected error: {str(e)}"
            }
    
    def perform_scientific_analysis(self, result: Dict, config: Dict) -> Dict:
        """ç§‘å­¦ç»Ÿè®¡åˆ†æ"""
        analysis = {}
        
        # 1. è§¦å‘ç‡æ£€éªŒï¼ˆäºŒé¡¹åˆ†å¸ƒï¼‰
        n = result['total_samples']
        p_expected = config['p_event']
        observed_triggers = result['event_triggered_count']
        
        if n > 0 and 0 < p_expected <= 1:
            p_actual = observed_triggers / n
            
            # è®¡ç®—99%ç½®ä¿¡åŒºé—´
            z_99 = 2.576  # 99%ç½®ä¿¡æ°´å¹³
            if p_expected > 0 and p_expected < 1:
                std_error = math.sqrt(p_expected * (1 - p_expected) / n)
                margin_error = z_99 * std_error
                ci_lower = p_expected - margin_error
                ci_upper = p_expected + margin_error
                
                analysis['trigger_rate_99_ci'] = (ci_lower, ci_upper)
                analysis['trigger_rate_in_99_ci'] = ci_lower <= p_actual <= ci_upper
                analysis['trigger_rate_deviation'] = abs(p_actual - p_expected)
        
        # 2. Categoryåˆ†å¸ƒå‡åŒ€æ€§æ£€éªŒï¼ˆå¡æ–¹æ£€éªŒï¼‰
        if config['control_category'] == UNCONTROL and result['event_triggered_count'] > 0:
            expected_per_category = result['event_triggered_count'] / len(ALL_CATEGORIES)
            chi_square_cat = 0
            
            for category in ALL_CATEGORIES:
                observed = result['category_distribution'].get(category, 0)
                chi_square_cat += (observed - expected_per_category) ** 2 / expected_per_category
            
            # è‡ªç”±åº¦ = ç±»åˆ«æ•° - 1
            df_cat = len(ALL_CATEGORIES) - 1
            critical_99_cat = 13.277  # df=4, 99%
            
            analysis['category_chi_square'] = chi_square_cat
            analysis['category_df'] = df_cat
            analysis['category_passes_chi_square_99'] = chi_square_cat < critical_99_cat
        
        # 3. Pressureåˆ†å¸ƒå‡åŒ€æ€§æ£€éªŒ
        if config['control_pressure_level'] == UNCONTROL and result['event_triggered_count'] > 0:
            expected_per_pressure = result['event_triggered_count'] / len(ALL_PRESSURE_LEVELS)
            chi_square_pres = 0
            
            for pressure in ALL_PRESSURE_LEVELS:
                observed = result['pressure_distribution'].get(pressure, 0)
                chi_square_pres += (observed - expected_per_pressure) ** 2 / expected_per_pressure
            
            df_pres = len(ALL_PRESSURE_LEVELS) - 1
            critical_99_pres = 11.345  # df=3, 99%
            
            analysis['pressure_chi_square'] = chi_square_pres
            analysis['pressure_df'] = df_pres
            analysis['pressure_passes_chi_square_99'] = chi_square_pres < critical_99_pres
        
        # 4. Phaseå‡åŒ€æ€§æ£€éªŒï¼ˆä¸¤ä¸ªphaseåº”è¯¥å¤§è‡´ç›¸ç­‰ï¼‰
        phase_counts = {phase: sum(counts.values()) for phase, counts in result['phase_distribution'].items()}
        if len(phase_counts) >= 2:
            total_phase_events = sum(phase_counts.values())
            if total_phase_events > 0:
                phase_deviations = []
                expected_per_phase = total_phase_events / len(phase_counts)
                
                for phase, count in phase_counts.items():
                    deviation = abs(count - expected_per_phase) / expected_per_phase
                    phase_deviations.append(deviation)
                
                analysis['max_phase_deviation'] = max(phase_deviations) if phase_deviations else 0
                analysis['phase_balanced'] = analysis['max_phase_deviation'] < 0.1  # 10%å®¹å·®
        
        return analysis
    
    def test_reproducibility(self, base_config: Dict) -> Dict:
        """æµ‹è¯•å¯é‡ç°æ€§"""
        print("ğŸ”„ Testing reproducibility with same seeds...")
        
        # ç”¨ç›¸åŒç§å­ç”Ÿæˆå¤šæ¬¡ï¼Œåº”è¯¥å®Œå…¨ç›¸åŒ
        test_seed = 12345
        sequences = []
        
        for run in range(5):
            event_system = EventSystem(
                TASK_STREAM_NAME,
                p_event=1.0,  # ç¡®ä¿æ‰€æœ‰äº‹ä»¶éƒ½è§¦å‘
                control_category=UNCONTROL,
                control_pressure_level=UNCONTROL,
                seed=test_seed,
                total_tasks=TOTAL_TASKS
            )
            
            sequence = []
            for task_num in range(1, TOTAL_TASKS + 1):
                phase = self.task_stream.get_phase_for_task(task_num)
                event_obj, event_variant = event_system.get_event_and_variant_for_phase(phase, task_num)
                
                if event_obj and event_variant:
                    sequence.append({
                        'task': task_num,
                        'category': event_variant['category'],
                        'pressure': event_variant['pressure_level'],
                        'event_name': event_variant['name']
                    })
            
            sequences.append(sequence)
        
        # éªŒè¯æ‰€æœ‰åºåˆ—ç›¸åŒ
        all_identical = all(seq == sequences[0] for seq in sequences)
        
        # ç”¨ä¸åŒç§å­ï¼Œåº”è¯¥äº§ç”Ÿä¸åŒç»“æœ
        different_seed_system = EventSystem(
            TASK_STREAM_NAME,
            p_event=1.0,
            control_category=UNCONTROL,
            control_pressure_level=UNCONTROL,
            seed=test_seed + 1,  # ä¸åŒç§å­
            total_tasks=TOTAL_TASKS
        )
        
        different_sequence = []
        for task_num in range(1, TOTAL_TASKS + 1):
            phase = self.task_stream.get_phase_for_task(task_num)
            event_obj, event_variant = different_seed_system.get_event_and_variant_for_phase(phase, task_num)
            
            if event_obj and event_variant:
                different_sequence.append({
                    'task': task_num,
                    'category': event_variant['category'],
                    'pressure': event_variant['pressure_level'],
                    'event_name': event_variant['name']
                })
        
        sequences_different = different_sequence != sequences[0]
        
        return {
            'same_seed_reproducible': all_identical,
            'different_seed_produces_different_results': sequences_different,
            'sequence_length': len(sequences[0]),
            'test_runs': len(sequences)
        }
    
    def run_complete_scientific_test(self) -> Dict:
        """è¿è¡Œå®Œæ•´çš„ç§‘å­¦æµ‹è¯•"""
        print("ğŸ”¬ Starting Scientific Rigorous Test Suite")
        print("=" * 70)
        
        start_time = time.time()
        
        # 1. ç”Ÿæˆæµ‹è¯•çŸ©é˜µ
        test_matrix = self.generate_scientific_test_matrix()
        print(f"ğŸ“‹ Generated {len(test_matrix)} test cases:")
        print(f"   - Scientific core tests: {len([t for t in test_matrix if t['test_type'] == 'scientific_core'])}")
        print(f"   - Engineering boundary tests: {len([t for t in test_matrix if t['test_type'] == 'engineering_boundary'])}")
        print(f"   - Weird bug detection tests: {len([t for t in test_matrix if t['test_type'] == 'weird_bug_detection'])}")
        print()
        
        # 2. è¿è¡Œæ‰€æœ‰æµ‹è¯•
        all_results = {}
        for i, config in enumerate(test_matrix):
            print(f"[{i+1:2d}/{len(test_matrix)}] ", end="")
            result = self.run_statistical_distribution_test(config)
            all_results[config['test_id']] = result
            
            if result['success']:
                print("âœ…")
            else:
                print(f"âŒ {result.get('error', 'Unknown error')}")
        
        # 3. å¯é‡ç°æ€§æµ‹è¯•
        print()
        reproducibility_result = self.test_reproducibility({})
        all_results['reproducibility'] = reproducibility_result
        
        total_time = time.time() - start_time
        
        return {
            'test_matrix': test_matrix,
            'results': all_results,
            'total_execution_time': total_time,
            'reproducibility': reproducibility_result
        }
    
    def generate_scientific_report(self, full_results: Dict) -> str:
        """ç”Ÿæˆç§‘å­¦æµ‹è¯•æŠ¥å‘Š"""
        report = []
        report.append("ğŸ”¬ Scientific Rigorous Test Report")
        report.append("=" * 70)
        report.append(f"Test Date: {datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report.append(f"Sample Size per Test: {SCIENTIFIC_SAMPLE_SIZE:,}")
        report.append(f"Significance Level: {SIGNIFICANCE_LEVEL}")
        report.append("")
        
        results = full_results['results']
        
        # æ€»ä½“ç»Ÿè®¡
        total_tests = len([r for r in results.values() if isinstance(r, dict) and 'test_id' in r])
        passed_tests = len([r for r in results.values() if isinstance(r, dict) and r.get('success', False)])
        
        report.append("ğŸ“Š Overall Results:")
        report.append(f"  Total Tests: {total_tests}")
        report.append(f"  Passed: {passed_tests}")
        report.append(f"  Failed: {total_tests - passed_tests}")
        report.append(f"  Success Rate: {passed_tests/total_tests*100:.1f}%")
        report.append("")
        
        # ç§‘å­¦æ ¸å¿ƒæµ‹è¯•ç»“æœ
        core_tests = [r for r in results.values() if isinstance(r, dict) and r.get('config', {}).get('test_type') == 'scientific_core']
        if core_tests:
            report.append("ğŸ§ª Scientific Core Tests:")
            passed_chi_square_cat = sum(1 for r in core_tests if r.get('category_passes_chi_square_99', True))
            passed_chi_square_pres = sum(1 for r in core_tests if r.get('pressure_passes_chi_square_99', True))
            passed_trigger_rate = sum(1 for r in core_tests if r.get('trigger_rate_in_99_ci', True))
            
            report.append(f"  Category Distribution Tests: {passed_chi_square_cat}/{len(core_tests)} passed")
            report.append(f"  Pressure Distribution Tests: {passed_chi_square_pres}/{len(core_tests)} passed")
            report.append(f"  Trigger Rate Tests: {passed_trigger_rate}/{len(core_tests)} passed")
            report.append("")
        
        # è¾¹ç•Œæµ‹è¯•ç»“æœ
        boundary_tests = [r for r in results.values() if isinstance(r, dict) and r.get('config', {}).get('test_type') == 'engineering_boundary']
        if boundary_tests:
            passed_boundary = len([r for r in boundary_tests if r.get('success', False)])
            report.append("âš™ï¸  Engineering Boundary Tests:")
            report.append(f"  Passed: {passed_boundary}/{len(boundary_tests)}")
            report.append("")
        
        # Bugæ£€æµ‹ç»“æœ
        bug_tests = [r for r in results.values() if isinstance(r, dict) and r.get('config', {}).get('test_type') == 'weird_bug_detection']
        if bug_tests:
            passed_bug = len([r for r in bug_tests if r.get('success', False)])
            report.append("ğŸ› Weird Bug Detection Tests:")
            report.append(f"  Correctly Caught Errors: {passed_bug}/{len(bug_tests)}")
            report.append("")
        
        # å¯é‡ç°æ€§ç»“æœ
        repro = results.get('reproducibility', {})
        if repro:
            report.append("ğŸ”„ Reproducibility Test:")
            report.append(f"  Same Seed Reproducible: {'âœ…' if repro.get('same_seed_reproducible') else 'âŒ'}")
            report.append(f"  Different Seeds Different: {'âœ…' if repro.get('different_seed_produces_different_results') else 'âŒ'}")
            report.append("")
        
        # å¤±è´¥çš„æµ‹è¯•è¯¦æƒ…
        failed_tests = [r for r in results.values() if isinstance(r, dict) and not r.get('success', True)]
        if failed_tests:
            report.append("âŒ Failed Tests:")
            for failed in failed_tests[:5]:  # åªæ˜¾ç¤ºå‰5ä¸ª
                report.append(f"  Test {failed.get('test_id', '?')}: {failed.get('error', 'Unknown error')}")
            if len(failed_tests) > 5:
                report.append(f"  ... and {len(failed_tests) - 5} more failed tests")
            report.append("")
        
        # æ‰§è¡Œæ—¶é—´
        total_time = full_results.get('total_execution_time', 0)
        report.append(f"â±ï¸  Execution Time: {total_time:.2f} seconds")
        report.append("")
        
        # ç§‘å­¦ç»“è®º
        if total_tests > 0:
            success_rate = passed_tests / total_tests
            if success_rate >= 0.95:
                report.append("âœ… SCIENTIFIC CONCLUSION: System passes rigorous testing")
                report.append("   - Statistical distributions are correct")
                report.append("   - Control mechanisms work properly")  
                report.append("   - Engineering edge cases handled correctly")
                report.append("   - System is suitable for scientific experiments")
            else:
                report.append("âŒ SCIENTIFIC CONCLUSION: System has significant issues")
                report.append("   - Statistical reliability is questionable")
                report.append("   - Not recommended for scientific experiments until fixed")
        
        return "\n".join(report)

def main():
    """ç§‘å­¦ä¸¥è°¨æµ‹è¯•ä¸»å‡½æ•°"""
    tester = ScientificRigorousTester()
    
    # è¿è¡Œå®Œæ•´æµ‹è¯•
    results = tester.run_complete_scientific_test()
    
    # ç”ŸæˆæŠ¥å‘Š
    report = tester.generate_scientific_report(results)
    
    # è¾“å‡ºæŠ¥å‘Š
    print("\n" + "=" * 70)
    print(report)
    
    # ä¿å­˜ç»“æœ
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    
    json_filename = f"TEST/scientific_rigorous_results_{timestamp}.json"
    with open(json_filename, 'w', encoding='utf-8') as f:
        json.dump(results, f, indent=2, ensure_ascii=False, default=str)
    
    report_filename = f"TEST/scientific_rigorous_report_{timestamp}.txt"
    with open(report_filename, 'w', encoding='utf-8') as f:
        f.write(report)
    
    print(f"ğŸ’¾ Results saved to: {json_filename}")
    print(f"ğŸ“„ Report saved to: {report_filename}")

if __name__ == "__main__":
    main()