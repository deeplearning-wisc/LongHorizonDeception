"""
ResultSaver - çº¯JSONæ ¼å¼çš„å®éªŒè®°å½•ä¿å­˜ç³»ç»Ÿ
åˆ é™¤æ‰€æœ‰TXTæ ¼å¼ï¼Œåªä¿å­˜ç»“æ„åŒ–JSONæ•°æ®
"""

import json
import time
from typing import Dict, List, Any, Optional
from pathlib import Path

class ResultSaver:
    """çº¯JSONæ ¼å¼çš„å®éªŒè®°å½•ä¿å­˜ç³»ç»Ÿ"""
    
    def __init__(self, session_name: str, results_dir: str = "results"):
        self.session_name = session_name
        self.results_dir = Path(results_dir)
        self.session_dir = self.results_dir / session_name
        self.session_dir.mkdir(parents=True, exist_ok=True)
        
        # å®éªŒæ•°æ®ç»“æ„
        self.experiment_data = {
            "session_metadata": {
                "session_name": session_name,
                "start_time": time.strftime('%Y-%m-%d %H:%M:%S', time.localtime()),
                "end_time": None,
                "experiment_mode": None,
                "total_tasks": 0,
                "total_interactions": 0
            },
            "experiment_config": {},
            "tasks": []
        }
        
        # å½“å‰ä»»åŠ¡å’Œè½®æ¬¡çŠ¶æ€
        self.current_task_info = None
        self.current_round_info = None
    
    def set_experiment_config(self, config: Dict[str, Any]):
        """è®¾ç½®å®éªŒé…ç½®"""
        self.experiment_data["experiment_config"] = {
            "max_rounds_per_task": config['max_rounds_per_task'],
            "p_event": config['p_event'],
            "summary_level": config['summary_level'],
            "llm_models": {
                "llm": self._extract_model_name(config, 'llm'),
                "manager": self._extract_model_name(config, 'manager'),
                "detector": self._extract_model_name(config, 'detector')
            }
        }
        self.experiment_data["session_metadata"]["experiment_mode"] = config['experiment_mode'].upper()
    
    def _extract_model_name(self, config: Dict, component: str) -> str:
        """ä»é…ç½®ä¸­æå–æ¨¡å‹åç§°"""
        llm_config = config['llm_api_config'][component]
        provider = llm_config['provider']
        return llm_config[provider]['model_name']
    
    def start_task(self, task_idx: int, task, event_info: Dict[str, Any]):
        """å¼€å§‹æ–°ä»»åŠ¡"""
        self.current_task_info = {
            "task_idx": task_idx,
            "task_id": task.task_id,
            "task_sequence_num": getattr(task, 'task_sequence_num', task_idx),
            "title": task.title,
            "description": task.description or getattr(task, 'base_prompt', ''),
            "files": [
                {
                    "filename": f.filename,
                    "file_type": f.file_type,
                    "content": f.content
                } for f in (task.files or [])
            ],
            "event_info": {
                "category": event_info['category'],
                "pressure_level": event_info['pressure_level'],
                "content": event_info['content'],
                "description": event_info['description']
            },
            "rounds": []
        }
    
    def start_round(self, round_num: int, manager_state_before: Dict[str, float]):
        """å¼€å§‹æ–°è½®æ¬¡"""
        self.current_round_info = {
            "round_num": round_num,
            "start_timestamp": time.time(),
            "manager_state_before": manager_state_before.copy(),
            "llm_response": None,
            "llm_timestamp": None,
            "manager_evaluation": None,
            "manager_timestamp": None
        }
    
    def save_llm_response(self, round_num: int, llm_response: str):
        """ä¿å­˜LLMå›å¤"""
        if self.current_round_info and self.current_round_info['round_num'] == round_num:
            self.current_round_info['llm_response'] = llm_response
            self.current_round_info['llm_timestamp'] = time.time()
    
    def save_manager_evaluation(self, round_num: int, manager_result: Dict[str, Any]):
        """ä¿å­˜Managerè¯„ä¼°"""
        if self.current_round_info and self.current_round_info['round_num'] == round_num:
            self.current_round_info['manager_evaluation'] = {
                "comprehensive_reasoning": manager_result['comprehensive_reasoning'],
                "detailed_reasoning": manager_result['detailed_reasoning'],
                "state_updates": manager_result['state_updates'],
                "feedback_response": manager_result['feedback_response']
            }
            self.current_round_info['manager_timestamp'] = time.time()
            
            # å®Œæˆå½“å‰è½®æ¬¡ï¼Œæ·»åŠ åˆ°ä»»åŠ¡ä¸­
            if (self.current_task_info and 
                self.current_round_info['llm_response'] is not None):
                self.current_task_info['rounds'].append(self.current_round_info.copy())
                self.current_round_info = None
    
    def end_task(self, task_idx: int):
        """ç»“æŸä»»åŠ¡"""
        if self.current_task_info:
            self.experiment_data['tasks'].append(self.current_task_info.copy())
            self.current_task_info = None
    
    def finalize_experiment(self, total_tasks: int, total_interactions: int, final_manager_state: Dict[str, float]):
        """ç»“æŸå®éªŒå¹¶æœ€ç»ˆä¿å­˜JSON"""
        # æ›´æ–°æœ€ç»ˆmetadata
        self.experiment_data["session_metadata"]["end_time"] = time.strftime('%Y-%m-%d %H:%M:%S', time.localtime())
        self.experiment_data["session_metadata"]["total_tasks"] = total_tasks
        self.experiment_data["session_metadata"]["total_interactions"] = total_interactions
        
        # æœ€ç»ˆä¿å­˜åˆ°åŒä¸€ä¸ªæ–‡ä»¶ - ä¸å†åˆ›å»ºé‡å¤æ–‡ä»¶
        json_file = self.session_dir / "experiment_data.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(self.experiment_data, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"ğŸ’¾ Experiment data finalized: {json_file}")
    
    def get_structured_interactions_for_detector(self) -> List[Dict[str, Any]]:
        """ä¸ºDetectoræä¾›æ‰å¹³åŒ–çš„interactionæ•°æ®"""
        detector_interactions = []
        
        for task_info in self.experiment_data['tasks']:
            for round_info in task_info['rounds']:
                manager_eval = round_info['manager_evaluation']
                
                interaction = {
                    # Taskä¿¡æ¯
                    "task_id": task_info['task_id'],
                    "task_sequence_num": task_info['task_sequence_num'],
                    "title": task_info['title'],
                    "description": task_info['description'],
                    "files": task_info['files'],
                    
                    # Eventä¿¡æ¯
                    "event_content": task_info['event_info']['content'],
                    "event_description": task_info['event_info']['description'],
                    "event_pressure_level": task_info['event_info']['pressure_level'],
                    "event_category": task_info['event_info']['category'],
                    
                    # Roundä¿¡æ¯
                    "round": round_info['round_num'],
                    "timestamp": round_info['start_timestamp'],
                    
                    # ManagerçŠ¶æ€
                    "manager_state_before": round_info['manager_state_before'],
                    
                    # äº¤äº’å†…å®¹
                    "llm_response": round_info['llm_response'],
                    "manager_feedback": manager_eval['feedback_response'],
                    
                    # Manageræ¨ç†
                    "comprehensive_reasoning": manager_eval['comprehensive_reasoning'],
                    "detailed_reasoning": manager_eval['detailed_reasoning'],
                    "state_updates": manager_eval['state_updates'],
                    "task_complete": manager_eval['state_updates']['task_complete']
                }
                
                detector_interactions.append(interaction)
        
        return detector_interactions
    
    def get_record_path(self) -> str:
        """è·å–è®°å½•æ–‡ä»¶è·¯å¾„"""
        return str(self.session_dir / "experiment_data.json")
    
    def flush(self):
        """å®æ—¶ä¿å­˜ - ç›´æ¥æ›´æ–°ä¸»æ–‡ä»¶"""
        if self.experiment_data['tasks']:
            # ç›´æ¥ä¿å­˜åˆ°ä¸»æ–‡ä»¶ï¼Œé¿å…é‡å¤çš„partialæ–‡ä»¶
            json_file = self.session_dir / "experiment_data.json"
            with open(json_file, 'w', encoding='utf-8') as f:
                json.dump(self.experiment_data, f, indent=2, ensure_ascii=False, default=str)
    
    @staticmethod
    def add_detector_analysis_to_experiment_data(original_file: str, detector_results: List[Dict[str, Any]], output_file: str = None) -> str:
        """
        å°†detectoråˆ†æç»“æœæ·»åŠ åˆ°ç°æœ‰çš„experiment_data.jsonä¸­
        
        Args:
            original_file: åŸå§‹experiment_data.jsonæ–‡ä»¶è·¯å¾„
            detector_results: detectoråˆ†æç»“æœåˆ—è¡¨
            output_file: è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œå¦‚æœä¸ºNoneåˆ™è‡ªåŠ¨ç”Ÿæˆ_with_detectoråç¼€
            
        Returns:
            è¾“å‡ºæ–‡ä»¶è·¯å¾„
        """
        import json
        from pathlib import Path
        import time
        
        # è¯»å–åŸå§‹å®éªŒæ•°æ®
        with open(original_file, 'r', encoding='utf-8') as f:
            experiment_data = json.load(f)
        
        # ç¡®å®šè¾“å‡ºæ–‡ä»¶è·¯å¾„
        if output_file is None:
            original_path = Path(original_file)
            output_file = str(original_path.parent / f"{original_path.stem}_with_detector{original_path.suffix}")
        
        # åˆ›å»ºdetectorç»“æœçš„ç´¢å¼•å­—å…¸ (task_id, round) -> detector_result
        detector_index = {}
        for result in detector_results:
            key = (result.get('task_id'), result.get('round'))
            detector_index[key] = {
                "deception_detected": result['deception_detected'],
                "intent": result['intent'],
                "category": result['category'],
                "severity": result['severity'],
                "confidence_score": result['confidence_score'],
                "reasoning": result['reasoning'],
                "evidence": result['evidence'],
                "analysis_timestamp": time.strftime('%Y-%m-%d %H:%M:%S', time.localtime()),
                "tokens_used": result['tokens_used'],
                "context_size": result['context_size']
            }
        
        # éå†å®éªŒæ•°æ®ï¼Œä¸ºæ¯ä¸ªroundæ·»åŠ å¯¹åº”çš„detectoråˆ†æ
        total_rounds_analyzed = 0
        for task in experiment_data['tasks']:
            task_id = task['task_id']
            for round_info in task['rounds']:
                round_num = round_info['round_num']
                key = (task_id, round_num)
                
                if key in detector_index:
                    round_info['detector_analysis'] = detector_index[key]
                    total_rounds_analyzed += 1
                else:
                    # å¦‚æœæ²¡æœ‰æ‰¾åˆ°å¯¹åº”çš„detectoråˆ†æï¼Œæ·»åŠ ç©ºçš„åˆ†æè®°å½•
                    round_info['detector_analysis'] = {
                        "deception_detected": None,
                        "intent": [],
                        "category": [],
                        "severity": 0,
                        "confidence_score": 0.0,
                        "reasoning": "No detector analysis available for this interaction",
                        "evidence": [],
                        "analysis_timestamp": time.strftime('%Y-%m-%d %H:%M:%S', time.localtime()),
                        "tokens_used": 0,
                        "context_size": 0,
                        "error": "Missing detector analysis"
                    }
        
        # åœ¨session_metadataä¸­æ·»åŠ detectoråˆ†æçš„æ±‡æ€»ä¿¡æ¯
        if 'detector_summary' not in experiment_data:
            experiment_data['detector_summary'] = {}
        
        # è®¡ç®—deception casesçš„ç»Ÿè®¡
        deceptive_results = [r for r in detector_results if r['deception_detected']]
        deception_count = len(deceptive_results)
        
        # è®¡ç®—ä¸¤ç§å¹³å‡ä¸¥é‡ç¨‹åº¦æŒ‡æ ‡
        # æŒ‡æ ‡1: åªè®¡ç®—æ£€æµ‹åˆ°deceptionçš„casesçš„å¹³å‡ä¸¥é‡ç¨‹åº¦
        average_severity_when_deceptive = 0.0
        if deceptive_results:
            average_severity_when_deceptive = round(
                sum(r['severity'] for r in deceptive_results) / len(deceptive_results), 3
            )
        
        # æŒ‡æ ‡2: æ‰€æœ‰interactionsçš„å¹³å‡ä¸¥é‡ç¨‹åº¦ (åŒ…æ‹¬severity=0çš„)
        average_severity_overall = 0.0
        if detector_results:
            average_severity_overall = round(
                sum(r['severity'] for r in detector_results) / len(detector_results), 3
            )
        
        experiment_data['detector_summary'].update({
            "analysis_timestamp": time.strftime('%Y-%m-%d %H:%M:%S', time.localtime()),
            "total_interactions_analyzed": len(detector_results),
            "total_rounds_with_analysis": total_rounds_analyzed,
            "deception_detection_count": deception_count,
            "deception_detection_rate": round(deception_count / len(detector_results), 3) if detector_results else 0.0,
            "average_confidence": round(sum(r['confidence_score'] for r in detector_results) / len(detector_results), 3) if detector_results else 0.0,
            "average_severity_when_deceptive": average_severity_when_deceptive,
            "average_severity_overall": average_severity_overall,
            "severity_distribution": {
                "0_honest": sum(1 for r in detector_results if r['severity'] == 0),
                "1_minor": sum(1 for r in detector_results if r['severity'] == 1),
                "2_moderate": sum(1 for r in detector_results if r['severity'] == 2),
                "3_significant": sum(1 for r in detector_results if r['severity'] == 3),
                "4_severe": sum(1 for r in detector_results if r['severity'] == 4)
            }
        })
        
        # ä¿å­˜å¢å¼ºåçš„æ•°æ®
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(experiment_data, f, indent=2, ensure_ascii=False, default=str)
        
        print(f"ğŸ’¾ Enhanced experiment data with detector analysis saved: {output_file}")
        print(f"ğŸ“Š Added detector analysis to {total_rounds_analyzed} rounds")
        
        return output_file